
Catboost:
when a dataset is given as input to the catboost model it first randomizes the dataset.
Then it looks for the columns which are descrete, if the descrete category columns have only two categories then it assigns 1 and 0 to each category.
then it applies ordered target encoding to all the descrete columns with more than two categories. after encoding the categorical data then it looks for the continous 
data columns. for the continous data columns it converts each column into bins, if there are only two bins then it replaces with 1's and 0's and if the bins are more 
than two then we use ordered target encoding to categorize the bins.

we take the average of the output and subtract the average values from the actual values these are known as residuals.

Now we need to build the first tree, for that we start considering each feature.
lets say feature1 and we select a threshold and start splitting.
initially the output is zero for each child node. and suppose if a value goes into a child node based on threshold we take the average of the value + output.
then if the second value also goes into the same node then we take the average of those two values as the output this will continue for all the datapoints.
we take the leaf output  at each iteration for each datapoint and calculate the similarity score. the threshold which gets the highest cosine similarity score will be selected
for splitting.


-------------------------------------------------------------------------------------------------------------------------------------------
ordered target encoding:
formula:option_count+0.05/n+1
option_count=total number of values that are labeled as 1 or 0(1 if that particular datapoint has label 1)  for that particular category before the present data point
n=total number of values for that particular category before the present datapoint
lets say we are dealing with a binary classification problem.
the feature has three categories (green,blue,red)
for the first datapoint lets say its blue since there are no previous datapoints with blue and label 1: 0+0.05/0+1=0.05
for the second datapoint, lets say it is red since there are no previous datapoints with red colour and labelled as 1 : 0+0.05/0+1=0.05
for the third datapoint, lets say it is green since there are no previous datapoints with green colour : 0+0.05/0+1=0.05
for the fourth datapoint, if the datapoint is blue and labeled as 1:
 the algorithm will look if there are previous datapoints which are blue and labelled as 1:
	if yes: then option_count=1 and n=1
	if no: then option_count=0 and n=1
lets say it is 1 then: 1+0.05/1+1=1.05/2
for the fifth datapoint if the datapoint is red and labeled as 0:
then option_count=0, n=1 output: 0+0.05/1+1

https://www.youtube.com/watch?v=3Bg2XRFOTzg

















How do you differentiate models based on output prediction?
Class output: 
Algorithms like SVM and KNN create a class output. For instance, in a binary classification problem, the outputs will be either 0 or 1.
 However, today we have algorithms which can convert these class outputs to probability. But these algorithms are not well accepted by the statistics community.

Probability output: 
Algorithms like Logistic Regression, Random Forest, Gradient Boosting, Adaboost etc. give probability outputs. 
Converting probability outputs to class output is just a matter of creating a threshold probability.
--------------------------------------------------------------------------------------------------------- 
what is regression?
regression analysis is used estimating the relationships between a dependent variable and one or more independent variables.

Regression analysis consists of a set of machine learning methods that allow us to predict a continuous outcome variable (y)
based on the value of one or multiple predictor variables (x).


-------------------------------------------------------------------------------------------------------------------------------------
explain linear regression?

Linear regression is a Machine learning model which tries to find the relationship between dependent and independent variables 
linear regression usually predict a continous output value which is a dependent variable using various factors 
which are also known as independent variables.

It aims to find the best-fitting linear equation (line in two dimensions, hyperplane in higher dimensions) that explains the relationship between variables based on observed data.

In order to find the relationship linear regression uses various methods and one such method is OLS.

There are various methods which can be used by linear regression to find the best fit line or the relation between dependent and independent variables:

Ordinary Least Squares (OLS): 
------------------------------
This is the most widely used method for linear regression. 
It minimizes the sum of squared differences between the observed and predicted values to find the coefficients that best fit the data.

Its formula is:
y = c + m * x      ## Linear Equation

loss FUNCTION:
loss function=(1/n)* summation of i=1 to n(predicted(i)-actual(i))**2

We choose the above function to minimize. 
The difference between the predicted values and original value measures the error difference. 
We square the error difference and sum over all data points and divide that value by the total number of data points. 
This provides the average squared error over all the data points. 
Therefore, this cost function is also known as the Mean Squared Error(MSE) function.


Gradient Descent: 
~~~~~~~~~~~~~~~~~~
Gradient descent is an iterative optimization algorithm used to find the coefficients that minimize the cost function (usually the mean squared error). 
It's commonly used for large datasets or when OLS becomes computationally expensive.

Stochastic Gradient Descent (SGD): 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Similar to gradient descent, but it updates the coefficients after each data point, making it suitable for online or streaming data.


Note:there are many more.

---------------------------------------------------------------------------------------------------------------------------------------------------
OLS interview answer:

Purpose: OLS is a method used to find the best-fitting linear relationship between variables by minimizing the sum of squared differences between observed and predicted values.

Equation: It involves finding the slope (m) and intercept (b) of the linear equation Y = m*X + b that minimizes prediction errors.

Optimization: OLS minimizes the total squared error by adjusting m and b iteratively until the line best fits the data points.

Assumptions: OLS assumes normally distributed errors and linearity between variables, and it may perform poorly if these assumptions are violated.

Application: OLS is widely used for tasks like predicting outcomes (e.g., sales, prices) based on features, and its effectiveness is often assessed using metrics like R-squared.




what is OLS?
Ordinary Least Squares, is a statistical method that is often used in machine learning as well, particularly in linear regression tasks.
OLS finds linear relationship between the input features and the target variable:
 Y = β0 + β1X1 + β2X2 + ... + ε, similar to the statistical OLS.

while finding the best fit line, it also estimates the coefficients of all the variables in the linear equation.
The term "least squares" refers to the method of minimizing the sum of squared differences between the observed values and the values predicted by the linear equation. 
The minimization is done using a loss function. The most common loss function used in OLS is the Mean Squared Error (MSE).
The MSE is the average of the squared differences between the predicted target values and the actual target values.
The algorithm adjusts the coefficients (β0, β1, β2, etc.) iteratively to minimize the MSE.
Once the coefficients are determined, the model is said to be "fitted" to the data. 
This means that the algorithm has found the best-fitting linear relationship between the input features and the target variable based on the training data.
After fitting the model, you can evaluate its performance on new, unseen data.
Common evaluation metrics for regression tasks include the coefficient of determination (R-squared), root mean squared error (RMSE), and mean absolute error (MAE).

m=summation(X-X`)*(Y-Y`)/summation(X-X`)**2
b=Y`-mX

-----------------------------------------------------------------------------------------------------------------------------------
explain each term in OLS result Table?
"[0.025, 0.975]" represents the 95% confidence interval for each coefficient.
lets take an example:
coeff is 1.5 and if confidence interval is [0.8 , 2.3) then 95% of the times coeff lies between 0.8 and 2.3

-----------------------------------------------------------------------------------------------------------------

Types of linear regression:
Simple linear regression:

In simple linear regression, we aim to reveal the relationship between a single independent variable or you can say input, 
and a corresponding dependent variable or output. We can discuss this in a simple line as y = β0 +β1x+ε

Multiple linear regression:

In this type of linear regression, we always attempt to discover the relationship between two or more independent variables or inputs and the corresponding dependent variable or output 
and the independent variables can be either continuous or categorical. 
----------------------------------------------------------------------------------------------------
when to use linear regression model?

The first step in determining if a linear regression model is appropriate for a data set is plotting the data and evaluating it qualitatively. 
when you want to a model to predict continous output and the data is linearly seperable.
when you want the results to be fast irrespective of the accuracy.
when the dataset follows all the assumptions required by the model like homosedacity,normality,independence,linearity.
when you want to understand the impact of variables on the dependent variable, usually results are more interpretable in linear regression.
-------------------------------------------------------------------------------------------------------------------------
can we use linear regression if the data is non linear?
no because one of the assumptions of linear regression model is the for a given value of x, y should be linear.
But in such cases we will try to transform the non linear data to linear data by applying some transformations 
on the non linear data like boxcox,log transform,squraring,exponential etc..
-------------------------------------------------------------------------------------------------------------


What are the assumptions required for linear regression?
There are four major assumptions: 
1. Linearity: 
The relationship between X and the mean of Y is linear.
The relationship between the independent variables (predictors) and the dependent variable (response) is assumed to be linear. 
This means that the change in the dependent variable for a unit change in an independent variable is constant across all levels of that variable.

2. Normality of residuals: 
Normality of residuals refers to one of the key assumptions of linear regression, which states that the residuals (also known as errors) should follow a normal distribution. 
Residuals are the differences between the observed values of the dependent variable and the values predicted by the linear regression model. 

Many hypothesis tests and confidence intervals in statistics assume that the residuals are normally distributed. 
Violating this assumption could lead to incorrect conclusions in hypothesis testing or unreliable confidence intervals.

When residuals are normally distributed, linear regression estimators are often more efficient and have desirable statistical properties.

Confidence intervals for regression coefficients provide a range of values that are likely to include the true population parameter. 
For example, if you have a 95% confidence interval for a coefficient, it means you are 95% confident that the true value of the coefficient falls within that interval.

3. Multicollinearity:
Linear regression assumes that there is very little multicollinearity between independent features.
The independent variables should not be highly correlated with each other. 
High multicollinearity can lead to instability in coefficient estimates and reduced interpretability of the model.

4. Homoscedasticity. 
This means the variance should be same throughout the data.
Also known as constant variance, this assumption states that the variability of the residuals should remain roughly constant across all levels of the independent variables. 
In other words, the spread of the residuals should not change as the pr edicted values change.

5: Autocorreleation: 
linear regression analysis requires that there is little or no autocorrelation in the data. 
   Autocorrelation occurs when the residuals are not independent from each other.  In other words when the value of y(x+1) is not independent from the value of y(x).
   Independence between residuals and independent variables is necessary for valid hypothesis testing and confidence interval estimation.
  When residuals are independent of the independent variables, the estimated coefficients (slopes) of the model are unbiased. 
  This means that the average of the parameter estimates obtained from multiple samples will be very close to the true population values.


we can check the linearity of the data using the scatter plots. while check linearity we can find the outliers in the data and we need to remove them
because linear regression is sensitive to outliers.

we can find the normality of the data using residual plots or Q-Q plot. if the data is not normally distributed then we should try to make the data normally distributed,
by applying some transformations such as log transormation,squaring the datapoints etc..

Multicollinearity occurs when the independent variables are too highly correlated with each other.
The amount of multicollinearity can be found between various independent variables using VIF Technique, correlation matrix,Tolerance.

 1)Correlation matrix – when computing the matrix of Pearson’s Bivariate Correlation among all independent variables the correlation coefficients need to be smaller than 1.
2) Tolerance – the tolerance measures the influence of one independent variable on all other independent variables; the tolerance is calculated with an initial linear regression analysis.  
Tolerance is defined as T = 1 – R² for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the data and with T < 0.01 there certainly is.
3)Variance Inflation Factor (VIF) – the variance inflation factor of the linear regression is defined as VIF = 1/T. 
With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is certainly multicollinearity among the variables.



https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/
-----------------------------------------------------------------------------------------------------------------------------
what are the assumptions for residuals?
1.They must be independent of each other.
2.They should follow a normal distribution.
3.homoscedasticity

how do you find that?
we can check for those assumptions using residual plot.
In the residual plot there will be a line passing from 0. think of this as a regression line.
the values which are passing below are the regression line will pass below the zero line in regression line and vice versa for others.
the distance between the datapoint and the reference line is same as the difference between the regression line and the predicted value.


what variables to use in residual plots.
we plot independent variable on x-axis and residual on y-axis, we use this when the no. of independent variables in only 1.

what do we do when the dataset has multiple independent variables?
in that case we use predicted variables on the x-axis and the residual values on the y-axis

how do you interpret the residual plots?

linearity:
if the datapoints are spread below and above the regression line without any clear pattern, then the datapoints are linear.
if the datapoints are spread but they form a curve like strucuture then the datapoints are non-linear, then we can try using transformation(log,power,square,box-cox) or use
non-linear regression models.
after applying the transform then we can plot and then see if that helps.

homoscedasticity:
if the datapoints are not unevely spread and the variance is not same through out then we can say the data is suffering with heterocedasity.
this increases type-1 error.
what we can do is:
use pission regression
use weighted regression
transormation of datapoint

Normality of residuals:
we can plot residuals using a histogram and see if they are normally distributed
we can also use Q-Q plot and see if all the datapoints are close to the line and should'nt be away from the line.

we can use transformations
sometimes doing transormation may also not work.
in general normality assumption is less important than others.
if the residuals are not normal, it means that the data is missing explainatory variable or due to presence of outliers.


Independence or Autocorrelation:
if we are using some datapoints which are collected based on time like monthly data.
each datapoint must be independent of each other.
they should not be any clear pattern like high values during some months and low values during other months.
we can also use durbin-watson test to find it- usually the values are between 0 and 4 if they are close to 2 that means the datapoints are uncorrelated.
if the values are less than 1 and greater than 3 then there is some problem with the dataset.


Multicollinearity:
it is a problem as there will be problem in calculating confidence intervals of coeff.
we can use VIF and remove the one with high VIF. one problem with this is we may lose some valuable information in that case we can combine 
two variables by taking ratios.
we can also use pca to solve this.



---------------------------------------------------------------------------------------------------------------------------------------------------------------

if the datapoints are randomly scattered around the line then the linear model is a good fit.
if the datapoints form a curve like structure then the model is not a good fit.
if the number of datapoints are less when the x is small and are more scattered when x increases then it is not a good fit as the variance is not same.
if the number of datapoints are high or more scattered when x is small and are less as x increases then the model is not a good fit as the variance is not same. 








2.Advantages of linear regression:

Linear regression performs exceptionally well for linearly separable data
Easy to implement and train the model
It can handle overfitting using dimensionlity reduction techniques and cross validation and regularization.
it is fast compared to other algorithms.

3. Disadvantages of linear regression:
Sometimes Lot of Feature Engineering Is required
If the independent features are correlated it may affect performance
It is often quite prone to noise and overfitting
it doesn't perform well when there are outliers and when there are missing values.

4. Whether Feature Scaling is required?
Yes
Because whenever we are talking or using optimizer, loss function there we need to scale the data
In ANN also we do feature scaling because we are using Gradient descent optimization
If we dont do feature scaling our gradients will be very much bigger
If we do feature scaling we can reach the global minima in a very short time.

5. Impact of Missing Values?
It is sensitive to missing values
we need to handle missing values by using some feature engineering Techniques.

6. Impact of outliers?
It is also important to check for outliers since linear regression is sensitive to outlier effects.
Due to outliers the regression line slope changes which eventually leads to wrong predictions
linear regression needs the relationship between the independent and dependent variables to be linear.

7.HyperParameter Tuning
Ridge Regression
Lasso Regression

8. Perfomance metrics
R.Square
Adjusted R.Square
MSE
MAE
RMSE

-----------------------------------------------------------------------------------------------------------
how do you interpret the results of a linear regression model?
There are few metrics we need to check while interpreting the values of OLS model.

R-square:
R-squared is a measure of how well the model fits the data. It represents the proportion of variance in the dependent variable explained by the independent variables in the model.
the R-square value must be high or close to 1, adjusted R-square value must be close to R-square value.
If the R-squared (R²) and adjusted R-squared values are not close to each other, it often indicates that the model may be suffering from a problem of overfitting or underfitting. 
If R-squared is significantly higher than adjusted R-squared, it might suggest overfitting(incase the model contains unnecessary features)
If R-squared is significantly lower than adjusted R-squared, it might suggest underfitting.


P-values:
P-values associated with coefficients indicate the statistical significance of each coefficient.
A small p-value (usually below 0.05) suggests that the coefficient is likely not zero, meaning the variable has a significant impact on the dependent variable.

F-statistic:
There is also one another variable known as F-statistic(F-Test) and prob(F-statistic) which helps us know  the significance of the overall regression model

It will try to compare the two models here.
one model only with the intercept value and another model included with input features,intercept.
we perform hypothesis on that where null hypothesis means two models are equal and alternate hypothesis means intercept only model is worse than the other model.


if F-statistic(F-Test) value is large and prob(F-statistic) i.e p value is close to 0 it means we reject the null hypothesis.
which means features are playing important role for predicting the target.

T-test:
we can also check for t-test values to ensure which variables are contributing more towards the target.
T-test will be performed on target and feature-x independently without considering the relation with other feature.
NUll Hypothesis: Feature-x intercept value is close or equal to zero
Alternate  Hypothesis: Feature-x intercept value is close or equal to zero

we will have T-value and p-value related to T-test
if the T-value is high  and p value is low  we reject null hypothesis.
if the p value is high for any feature, it means that it is irrelevant for that model or it is not contributing much to the target variable.

https://www.youtube.com/watch?v=U7D1h5bbpcs
---------------------------------------------------------------------------------------------------------- 

WHAT IS R-SQUARED?
R-squared (R²) is a number we use in regression analysis to see how well a model fits the data we have. 
It helps us understand if the model's predictions match the actual data points.
 It provides information about how well the independent variables (predictors) explain the variability in the dependent variable (outcome or response variable). 
It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. .

In simpler terms, R-squared tells you the proportion of the variance in the dependent variable that can be explained by the independent variables in your regression model. 
It ranges between 0 and 1

R-squared = Explained variation / Total variation

R-squared is always between 0 and 100% i.e btw 0 to 1:

R-square is closer to 1, it means the model's predictions are very close to the actual data points. 
If it's closer to 0, the model's predictions are not as good at explaining the data.

0% indicates that the model explains none of the variability of the response data around its mean.
100% indicates that the model explains all the variability of the response data around its mean.

R-SQUARED=1-(explained variation/Total variation)=1-RSS/TSS
where RSS is sum of the squared difference between actual values and predicted values.
TSS is the sum of the squared differences between the actual data points and the mean of those actual data points


example:
suppose if you are calculating revenue of ice cream based on temperature
R**2 for temp is 80% this means 80% of revenue of ice crease is because of temperature.

suppose if we add one more feature to it then R**2 value will go up to some level lets say 85%.
In the same way as you go on adding features the R**2 goes up irrespetive of whether the feature is contributing or not.

However, a high R² value doesn't necessarily mean that the model is a good fit. 
This is because R² increases as more independent variables are added to the model, 
even if those variables don't have a strong relationship with the dependent variable. 
Therefore, it's important to interpret R² in the context of the specific problem and consider other metrics and analyses as well.

This is the basic problem with R-Square. How many junk independent variables  you add to your model, the R-Squared value will
always increase. It will never decrease with the addition of a newly independent variable, whether it could
be an impactful, non-impactful, or bad variable, so we need another way to measure equivalent RSquare,
which penalizes our model with any junk independent variable.

why rsquare value increases is in a regression model when you add a new feature one coeffecient value will added for 2 features
y=mx1+mx2+c
for 3 features 
y=mx1+mx2+mx3+c
so for 3 features we added one m and x3 
what it does is value of RSS will decrease and Rsquare value will increase.

So, we calculate the Adjusted R-Square with a better adjustment in the formula of generic R-square.

adjusted R**2=1-(1-R**2)(N-1)/N-p-1, where N is no of observations and p is no. of independent features.

the denominator value with the increase in p value and hence the (1-R**2)(N-1)/N-p-1 will be more because of which adjusted sqr will be less
adjusted r sqr only decrease if the added feature is not correlated or not contributing.


Adjusted r square overcomes the Rsqure issue by adding a penalty if we are trying to add a feature that is contributing less.
IF useless features are added Adjusted Rsqure will decrease
If useful features are add Adjusted Rsqure will increase.

https://www.youtube.com/watch?v=WuuyD3Yr-js&t=321s
https://statisticsbyjim.com/regression/interpret-adjusted-r-squared-predicted-r-squared-regression/
--------------------------------------------------------------------------------------------------------------
Explain How adjusted rsquare tells whether newly added variable has an impact or not?
Imagine you have a dataset of houses and you're trying to predict their prices based on the size of the house.
You calculate the adjusted R-squared and find it to be 0.75
now when you add a variable which has impact and see that Rsquare is 0.82 and adjusted rsquare is 0.79, the adjusted rsquare is high compared to previous value which
means that added variable has an impact.
now when you add a variable which doesn't have an impact and has an rsquare of 0.76 and adjusted rsquare of 0.73, the adjusted rsquare is low compared to the previous value
which means the added variable doesn't have much impact.

--------------------------------------------------------------------------------------------------------------------------------------
Difference between cost function and loss function:
"loss function" and "cost function" are often used interchangeably, "loss function" specifically refers to the function that measures the error between predicted and 
actual values for a single data point, while "cost function" refers to the function that computes the average error across the entire dataset. 

---------------------------------------------------------------------------------------------------------------------
Can a linear regression model used for binary classification?
When we try to use a linear regression to a binary classification:
first of all the data should satisfy all the assumptions of linear regression, since the dataset is a classification one it may not satisfy.
The values which the model predicts are continous and there is no range for it, if we want to apply a threshold we can't say the threshold is an optmial value to optmize.
The dataset can be imbalanced, there might be non linear relationship and linear regression cannot handle it.
we can use the linear regression model for binary classifcation but the output will be contionous.
we can also make use of Threshold to classify.

The cost function is also different in linear regression. we use MSE which works better when the output data is continous.
when we use MSE we get a lot of local minimas and the cost function also doesn't decrease.
example:
actual=1 predicted=2.5 then (1-2.5)sqr=(-1.5)sqr=2.25
actual=1 predicted=-.5 then (1-(-.5)sqr=(2)sqr=4.

when we use LOG LOSS of binary cross entropy:
suppose for actual value of 1 if the predicted prob is close to 1 then log(1) will be zero it means low cost value if pred prob is close to 0
then log(0) will be 1.

for actual value of 0 if the predicted prob is close to 1 then 1-log(1) will be 1 it means high cost value if pred prob is close to 0
then 1-log(0) will be 0 which means close to 0 and less cost.


https://jinglescode.github.io/2019/05/07/why-linear-regression-is-not-suitable-for-classification/
----------------------------------------------------------------------------------------------------------
Activation Function in Linear Regression?

A neural network without an activation function is essentially just a linear regression model. 
The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. 
a(1) is the vectorized form of any linear function.
----------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------

what is goodness of fit for a linear regression model? 


Regression helps in determining the relation between two variables, sometimes it is able to find there relationship quite well and sometimes it does not.
we must be able to distinguish between the cases for that we use standard error of estimate(SEE) helps in finding the goodness of fit test.

it is similar to that of a standard deviation of a single variable but here it measures the standard deviation of the error terms or also known as residuals.


https://www.youtube.com/watch?v=fCgwSEFlnt8


A goodness-of-fit test, in general, refers to measuring how well do the observed data correspond to the fitted (assumed) model.
Like in a linear regression, in essence, the goodness-of-fit test compares the observed values to the expected (fitted or predicted) values.
---------------------------------------------------------------------------------------------------------------------------------










------------------------------------------------------------------------------------------------------------------------------
what is the main of any algorithm?
Main aim is to reduce the cost function less the cost function higher will be the accuracy.

--------------------------------------------------------------------------------------------
hat is Mean Square Error?
Mean square error is a perfomance metrix which is used to measure the perfomance of a regression model.
The mean squared error tells you how close a regression line is to a set of points. 
It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them.

The line equation is y=Mx+B. We want to find M (slope) and B (y-intercept) that minimizes the
squared error.

---------------------------------------------------------------------------------------------------------------
what is noise?

The training data may contain data points that do not accurately represent the properties of the data. These points are considered as noise.

when does a model overfit?
Overfitting occurs because a model fails to generalize the data that contains a lot of irrelevant data points. 
data points that do not reflect the properties of the data are considered to be irrelevant. 
----------------------------------------------------------------------------------------------------------------
what is regularization?
Regularization is a techinque in machine learning which adds a penalty term to the loss function during the training of a model.
It resolves the overfitting problem aand create a more generalized form of the model by reducing the complexity and magnitude of coefficients of variables.

In regularization what we do is it adds penalty term to the model what it does is adds information to a model to prevent the occurrence of overfitting.
It is a type of regression that minimizes the coefficient values of certain features to zero to reduce the capacity (size) of a model. 
In this context, the reduction of the capacity of a model involves the removal of extra weights.
Regularization removes extra weights from the selected features and redistributes the weights evenly.
This means that regularization discourages the learning of a model which has both high complexity and flexibility.
A highly flexible model is one that possesses the freedom to fit as many data points as possible.
we may judge the complexity of a predictive model by the number of features it possesses.
A model with a lot of features to learn from is at a greater risk of overfitting. 
By discouraging the learning of (or use of) highly complex and flexible models, the risk of overfitting is lowered.

Let’s use a linear regression equation to explain regularization further.

Y=β0+β1X1+β2X2+…+βpXp


Y represents the value that is to be predicted. βi stands for the regressor coefficient estimates for the corresponding predictor Xi. 
And, Xi represents the weights or magnitudes assigned to various predictors (independent variables). 
Here, i represents any value greater than or equal to 0, and less than p.

A loss function provides a means of assessing how well an algorithm models given data. It is used to minimize the error, in turn optimizing the weights. 
In this context, the loss function is referred to as the residual sum of squares (RSS).

Based on the training data, the loss function will adjust the coefficients. 
If the presence of noise or outliers is found in the training data, the approximated coefficients will not generalize well to the unseen data. 
Regularization comes into play and shrinks the learned estimates towards zero.

In other words, it tunes the loss function by adding a penalty term, that prevents excessive fluctuation of the coefficients. Thereby, reducing the chances of overfitting.


------------------------------------------------------------------------------------------------------------------------------------------------------

Lasso regression(Least Absolute shrinkage and selection operator):
lasso regression is a type of linear regression model which incluedes a regularization term.
we can say lasso regression as an improved version of linear regression  
you should use lasso when you suspect there are unimportant factors in your data that you want the model to ignore.
It's like tidying up a messy room – if some things aren't really useful, lasso helps in getting rid of them, making the final result simpler and easier to understand. 
Lasso regression is a regularization technique used to reduce model complexity. It is also known as L1 regularization. 
Lasso stands for Least Absolute Shrinkage and Selector Operator.
The penalty term added is the sum of the absolute value of coefficients. alpha controls the strenth of the penalty term.

Let’s look at the equation below:

∑i=1to n(yi–β0 − ∑j=1to p βj*xij)2+λ∑j=1top ∣βj∣  ==RSS+λ∑j=1p∣βj∣

Lasso minimizes the regression coefficients to regularize the model parameters. Sometimes, 
Lasso can reduce regression coefficients to zero, which is particularly important when it comes to feature selection.

The predictors whose coefficients are reduced to zero will not be included in the final model. 
These are the predictors considered to have less importance. This is how some features are eliminated. 
However, every non-zero regression coefficient is selected for use in the model. This greatly assists in minimizing prediction errors.
Lasso also helps improve the prediction accuracy of the models. The shrinking coefficients minimizes the bias and improves the variance of models.

when to use lasso?
When you're dealing with datasets that have a large number of features, Lasso can help you identify the most important features and exclude the less relevant ones.
When you want to prevent overfitting and improve your model's generalization to new data, Lasso's regularization can help control the complexity of the model.

when to not use lasso?
 it might not work well if all features are important, or if the relationships between features and the target are highly nonlinear. 
 In such cases, you might consider using other regularization techniques or nonlinear models.
 
 --------------------------------------------------------------------------------------------------------

Ridge Regression:
Ridge regression refers to a type of linear regression where we add a penalty term to it inorder to make it more accurate
It is also known as L2 regularization. 
Ridge regression can be used when the model is suffering with multicollinearity problem.
Ridge helps in managing the collinearity by shrinking the coefficients towards zero, but it doesn't eliminate any of them.
In Ridge regression, the penalty term added to the loss function is the squared sum of the coefficients of the model's features. This penalty term is multiplied by a parameter (
α) that determines the strength of regularization.  The goal of this penalty term is to encourage the coefficients to be small and to prevent them from growing too large, 
which helps in reducing overfitting and making the model more stable.


∑i=1n(yi–β0−∑j=1pβjxij)2+λ∑j=1pβ2j=RSS+λ∑j=1pβ2j
we add lamda*slope square to the cost function where lamda is the hyperparameter
what do we do in Ridge Regression?
when slope is more we use Ridge Regression.


As we can see, the main difference between the above equation and the general equation for the loss function is that λ∑j=1pβ2j contains the squared value of the regression coefficients. 
The tuning parameter is λ. Higher values of the coefficients represent a model with greater flexibility. 
To penalize the flexibility of our model, we use a tuning parameter that decides the extent of the penalty. To minimize the function, these coefficients should be small. 
L2 regularization ensures the coefficients do not rise too high.

Ridge regression adds an amount of bias to the regression estimates to reduce errors. Ridge regression is a potential solution to handle multicollinearity.


--------------------------------------------------------------------------------------------------------------------------------------
Difference between Lasso and Ridge:

The key difference between these two is the penalty term.

The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, 
removing some feature altogether. 
So, this works well for feature selection in case we have a huge number of features and we think not all the features are important.

When you want a simpler model with fewer non-zero coefficients, Lasso is preferable. It creates sparse models where only a subset of features have non-zero coefficients.
When you're interested in understanding which features contribute the most to the model's predictions, 
Lasso's tendency to select and emphasize important features can provide clearer insights.

When your dataset has highly correlated features, Ridge regression can be more effective than Lasso because it doesn't force coefficients to become exactly zero. 
Ridge helps in managing the collinearity by shrinking the coefficients towards zero, but it doesn't eliminate any of them.
If you believe that most or all of your features are relevant for prediction, Ridge regression might be a better choice. 
Lasso tends to push some coefficients to zero, potentially excluding even important features.

When your main goal is to improve predictive performance while controlling overfitting, Ridge can be preferred. 
It typically gives smoother coefficient estimates across features compared to Lasso.

Ridge regression has one clear disadvantage: model interpretability. When shrinking the coefficients of the predictors of least importance, it will reduce them to be close to zero.

As a result, one may end up including all the coefficients in the final model. The final model ends up containing the predictors one may prefer to eliminate. 
As such, no feature selection is done. Whereas lasso regression manages to force some coefficient estimates to zero when the λ is large enough. 
We can say lasso performs better feature selection.


For Ridge regression the meta-parameter is often called "alpha" or "L2"; it simply defines regularization strength. 
For LASSO the meta-parameter is often called "lambda", or "L1". 
In contrast to Ridge, the LASSO regularization will actually set less-important predictors to 0 and help you with choosing 
the predictors that can be left out of the model. 

It will shrink the coefficients for least important predictors, very close to zero. 
But it will never make them exactly zero. In other words, the final model will include all predictors.

 https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/
---------------------------------------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------
perfomance metrics in regression models? when to use each metrics?

Mean Absolute Error (MAE)
This is simply the average of the absolute difference between the target value and the value predicted by the model. 
Not preferred in cases where outliers are prominent.

Mean Squared Error (MSE)
The most common metric for regression tasks is MSE. It has a convex shape. 
It is the average of the squared difference between the predicted and actual value. 
Since it is differentiable and has a convex shape, it is easier to optimize.

R**2 can also be used when data has outliers.

-------------------------------------------------------------------------------------------------------------

explain logistic regression?
Logistic regression is basically a supervised classification algorithm which is widely used in classification problems.

For example, if you want to
predict whether a particular political leader should succeed or not. 
In this case, the end of the forecast is binary ie 0 or 1 (success / loss).

Logistic regression is a technique in which we predict the probability of an event happening (like 'yes' or 'no' outcomes) based on input features. 
It uses the sigmoid function to transform a linear combination of features into a probability value between 0 and 1. 
If the probability is above a threshold (usually 0.5), we predict one class; if below, we predict the other. 
It's commonly used for binary classification tasks and helps us understand relationships between features and outcomes.

-----------------------------------------------------------------------------------------------------------------------------

why is logistic regression a classification model and not used for regression tasks?

logistic regression is  a regression model. 
Here the logistic regression model tries  to predict the probability that a given data point belongs to a certain catogory. 
Logistic regression models the data using the sigmoid function.

Logistic regression becomes a classification technique only when a decision threshold is brought into the picture. 
The setting of the threshold value is a very important aspect of Logistic regression and is dependent on the classification problem itself.
-------------------------------------------------------------------------------------------------------------------------------------
when should we use Logistic regression?
we use logistic regression  for a classification problem.
when the data can be linearly seperable.
when you want the results to be fast irrespective of the accuracy and the results must be easily interpreted(understood)
when the number of observations are more than the number the number of features.

when we shouldn't use?
If the independent features are correlated
when the data cannot be linearly seperable.
If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting.
Non-linear problems can’t be solved with logistic regression because it has a linear decision surface. 
when there are outliers and missing values in the data.
-------------------------------------------------------------------------------------------------------------------------------------------
What is the Sigmoid Function?
we will get a straight line when we use linear function which will give a wide range of values.
We need a function to transform data output in such a way that values will be between 0 and 1

When using linear regression we used a formula of the hypothesis i.e.
hΘ(x) = β₀ + β₁X
For logistic regression we are going to modify it a little bit i.e.
σ(Z) = σ(β₀ + β₁X)
We have expected that our hypothesis will give values between 0 and 1.
Z = β₀ + β₁X
hΘ(x) = sigmoid(Z)
i.e. hΘ(x) = 1/(1 + e^-(β₀ + β₁X)
----------------------------------------------------------------------------------------------------------------------
can we use MSE as a cost function in logistic regression if yes why and if no then why?

In Logistic Regression Ŷi is a nonlinear function(Ŷ=1​/1+ e-z), if we use the output of sigmoid function in 
MSE equation for calculating loss it will give a non-convex function(shape not similar to a bowl but in the form of sinusoidal grapth)
with many local minimas and it will be difficult to find the global minima.
This strange outcome is due to the fact that in logistic regression we have the sigmoid function around, which is non-linear (i.e. not a line).

When we try to optimize values using gradient descent it will create complications to find global minima.

Another reason is in classification problems, we have target values like 0/1, So (Ŷ-Y)2 will always be in between 0-1 which can make it very difficult 
to keep track of the errors and it is difficult to store high precision floating numbers.

That's why we still need a neat convex function as we did for linear regression:
 a bowl-shaped function that eases the gradient descent function's work to converge to the optimal minimum point.
 
when we use a log loss function when the error rate is less output of logloss is small.
when we use a log loss function when the error rate is high output of logloss is very high.

https://www.geeksforgeeks.org/ml-cost-function-in-logistic-regression/
https://www.youtube.com/watch?v=ar8mUO3d05w
--------------------------------------------------------------------------------------------------------------------------

1. What Are the Basic Assumption?
Linear Relation between independent features and the log odds

2. Advantages
Advantages of Logistics Regression

Logistic Regression Are very easy to understand
It requires less training
Good accuracy for many simple data sets and it performs well when the dataset is linearly separable.
It makes no assumptions about distributions of classes in feature space.
Logistic regression is less inclined to over-fitting but it can overfit in high dimensional datasets.
One may consider Regularization (L1 and L2) techniques to avoid over-fitting in these scenarios.
Logistic regression is easier to implement, interpret, and very efficient to train.

3. Disadvantages
Sometimes Lot of Feature Engineering Is required
If the independent features are correlated it may affect performance
It is often quite prone to noise and overfitting
If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting.
Non-linear problems can’t be solved with logistic regression because it has a linear decision surface. 
Linearly separable data is rarely found in real-world scenarios.
It is tough to obtain complex relationships using logistic regression. 
More powerful and compact algorithms such as Neural Networks can easily outperform this algorithm.
In Linear Regression independent and dependent variables are related linearly. 
But Logistic Regression needs that independent variables are linearly related to the log odds (log(p/(1-p)).

4. Whether Feature Scaling is required?
yes

5. Missing Values
Sensitive to missing values

6. Impact of outliers?
Like linear regression, estimates of the logistic regression are sensitive to the unusual observations:
outliers, high leverage, and influential observations. 
Numerical examples and analysis are presented to demonstrate the most recent outlier diagnostic methods using data sets from medical domain

Types of Problems it can solve(Supervised)
Classification
Practical Implementation
http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

Performance Metrics
Classification
Confusion Matrix
Precision,Recall, F1 score
---------------------------------------------------------------------------------------------------------------
cost function used in Logistic regression?
The cost function which is used to optimize the error rate in logistic regression is called log loss or binary cross entropy.
cross-entropy loss, is used in logistic regression to measure the difference between predicted probabilities and actual class labels.

J(θ)=-summation(yi*log(hθ(x))+(1-yi)log(1−hθ(x)))

For logistic regression, the Cost function is defined as:
−log(hθ(x)) if y = 1
−log(1−hθ(x)) if y = 0

If we try to use the cost function of the linear regression in ‘Logistic Regression’ 
then it would be of no use as it would end up being a non-convex function with many local minimums, 
in which it would be very difficult to minimize the cost value and find the global minimum.


The above two functions can be compressed into a single function i.e.



One of the main reasons why MSE doesn't work with logistic regression is when MSE loss function is plotted with respect 
to weights of the logistic regression model, the curve obtained is not a convex curve which makes it very difficult to find the global minimum.
------------------------------------------------------------------------------------------------------------------------------
Perfomance metrics in Classification:
The main Aim in any classification problem should be to reduce the Type1 and Type2 Error.

confusion matrix:

A confusion matrix is a table that is often used to describe the performance of a classification model on a set of 
test data for which the true values are known.

True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the
 value of predicted class is also yes. E.g. 
 if actual class value indicates that this passenger survived and predicted class tells you the same thing.

True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class
 is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.

False Positives (FP) – When actual class is no and predicted class is yes. 
E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.
FP is also known as Type1 Error.

False Negatives (FN) – When actual class is yes but predicted class in no. 
E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.
FN is also known as Type2 Error.

Accuracy = TP+TN/TP+FP+FN+TN

when we have a balanced dataset then we will consider accuracy as the Perfomance metrics.
sometimes it also depends on the usecase of what metrics to use.


False Positive Rate=FP/FP+TN
-------------------------------------------------------------------------------------------
When the dataset is imbalanced we consider the below Perfomance Metrics.
suppose when the dataset is imbalanced in a binary classification problem like 90postive-10negative 
if the model predicted it as 100 positive and 0 negative accuracy would be 90% but the metrics which we choose is wrong.
because it incorrectly predicted the minority classes.
Our main aim should be to reduce FP and FN.

Precision :
Precision = TP/TP+FP.
Precision will be the perfomance metrics to use in a usecase where having a FP might be costly. 
Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. 
The question that this metric answer is of all passengers that labeled as survived, 
how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.

Having said above, in case of spam email detection, One should be okay if a spam email (positive case) left undetected and doesn't go 
to spam folder but, if an email is good (negative), then it must not go to spam folder. i.e. Precison is more important. 
(If model predicts something positive (i.e. spam), it better be spam. else, you may miss important emails).

When we have imbalanced class and we need high true positives, precision is prefered over recall. 
because precision has no false negative in its formula, which can impact.

Precision is more important than recall when you would like to have less False Positives in trade off to have more False Negatives. 
Meaning, getting a False Positive is very costly, and a False Negative is not as much.

------------------------------------------------------------------------------------------------------------------------
Recall (Sensitivity or TPR) - 
Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. 
recall is also known as TPR.
This is used when predicting poistive as negative is costly.
recall is more important than precision when you would like to have less False negatives in trade off to have more False positive.
recall = TP/TP+FN.

Recall will be considered as a perfomance metrics to use when the given usecase needs to have less no of FN or predicting a Negative value for actual
positive might cause problems.

usecase:
cancer detection -- 1-has cancer,0-no cancer.
suppose if a person is having a cancer but the model is predicting it as no cancer.
actual positive and model predicted negative which means False negative.
so Having False negative will be costly. in that we take use of Recall.

-------------------------------------------------------------------------------------------------------------

F-Beta score:
In an Imbalanced dataset there might be a situation where Both FP and FN both should be less in that case 
we use F1 score which is also known F Beta score.
suppose for a bank loan prediction problem.

F-Beta=(1+Beta squre)(precision*recall)/(Beta square*precision+Recall
if Beta us 1 then it is known as F1 score.
we select Beta=1 when FP and FN both are important.

suppose if FP has higher impact or FP is more important i.e Type1 error then we have Beta value between 0.1 to 0.5 or between 0 and 1
suppose if FN has higher impact or FN is more important i.e Type2 error then we have Beta value more than 1
--------------------------------------------------------------------------------------
F1 score - F1 Score is the weighted average of Precision and Recall. 
Therefore, this score takes both false positives and false negatives into account. 
Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, 
especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. 
If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. 
In our case, F1 score is 0.701.

F1 Score = 2*(Recall * Precision) / (Recall + Precision) this is known as Harmonic mean.

Recall = TP/TP+FN

If you want to know if your predictions are good, you need these two measures. You can have a precision of 1 (so when you say it's positive, 
it's actutally positive) but still have a very low recall (you predicted 3 good positives but forgot 15 others). 
Or you can have a good recall and a bad precision.

This is why you might check f1-score, but also any other type of f-score. If one of these two values decreases dramatically, 
the f-score also does. But be aware that in many problems, we prefer giving more weight to precision or to recall 
(in web security, it is better to wrongly block some good requests than to let go some bad ones).

--------------------------------------------------------------------------------------------------------------------

Specificity: it is opposite to that of recall
Specificity (true negative rate) is calculated as the ration of correctly predicted negative values to the total no of actual negative points.
specificity=TN/TN+FP

-------------------------------------------------------------------------------------
Difference between logistic and linear regression?
Answer:
Linear and Logistic regression are the most basic form of regression which are commonly used. 
The essential difference between these two is that Logistic regression is used when the dependent variable is binary. 
In contrast, Linear regression is used when the dependent variable is continuous, and the nature of the regression line is linear.
Key Differences between Linear and Logistic Regression
Linear regression models data using continuous numeric value. As against, logistic regression models the data in the binary values.
Linear regression requires to establish the linear relationship among dependent and independent variables, 
whereas it is not necessary for logistic regression.
In linear regression, the independent variable can be correlated with each other. 
On the contrary, in the logistic regression, the variable must not be correlated with each other.
logistic uses log loss, linear uses mse
logistic uses sigmoid, linear uses linear function
logistic output is binary, linear output is continous
logistic has only one assumption to follow, linear has 4 assumptions.
-----------------------------------------------------------------------------------------------------------------------------
Decision Boundary
decision boundary is a threshold value which we specify on a model that returns a probability score between 0 and 1.
We expect our classifier to give us a set of outputs or classes based on probability when we pass the inputs through a prediction function 
and returns a probability score between 0 and 1.For Example, We have 2 classes, let’s take them like cats and dogs(1 — dog , 0 — cats). 
We basically decide with a threshold value above which we classify values into Class 1 and of the value goes below the threshold 
then we classify it in Class 2.

if we have chosen the threshold as 0.5, if the prediction function returned a value of 0.7 then we would classify this observation as Class 1(DOG). 
If our prediction returned a value of 0.2 then we would classify the observation as Class 2(CAT).
----------------------------------------------------------------------------------------------------------------------------
what is heterocedasity?

heteroscedasticity is the condition in which the variance of error term or the residual term in a regression model varies.
Heteroscedasticity means data is not uniformly distributed. 


What causes Heteroscedasticity?
Heteroscedasticity occurs more often in datasets, where we have a large range between the largest and the smallest observed values. 
There are many reasons why heteroscedasticity can exist, and a generic explanation is that the error variance changes proportionally with a factor.


How to Fix Heteroscedasticity?

Weighted regression:
It is a method that assigns each data point to a weight based on the variance of its fitted value. 
The idea is to give small weights to observations associated with higher variances to shrink their squared residuals. 
Weighted regression minimizes the sum of the weighted squared residuals. When you use the correct weights, 
heteroscedasticity is replaced by homoscedasticity

---------------------------------------------------------------------------------------------------------------------------
What is multicollinearity, and how do you treat it?
Multicollinearity means independent variables are highly correlated to each other. 
In regression analysis, it's an important assumption that the regression model should not be faced with a problem of multicollinearity.
1.It makes it hard to interpret of model 
2.creates an overfitting problem.
3.It would be hard for you to choose the list of significant variables for the model if the model gives you different results every time.

If two explanatory variables are highly correlated, it's hard to tell, what is the effect of the individual effects on  the dependent variable
and the model results fluctuate significantly.
Let's say Y is regressed against X1 and X2 and where X1 and X2 are highly correlated. 
Then the effect of X1 on Y is hard to distinguish from the effect of X2 on Y because any increase in X1 tends to be associated with an increase in X2.
Another way to look at the multicollinearity problem is: Individual t-test P values can be misleading. 
It means a P-value can be high, which means the variable is not important, even though the variable is important.

we can check multicollinearity using correlation matrix and calculating VIF?
Depending on the situation, it may not be a problem for your model if only slight or moderate collinearity issue occurs. 
However, it is strongly advised to solve the issue if severe collinearity issue exists
(e.g. correlation >0.8 between 2 variables or Variance inflation factor(VIF) >20 )

Correcting Multicollinearity:
1) Remove one of the highly correlated independent variables from the model. If you have two or more factors with a high VIF, remove one from the model.
2) Principle Component Analysis (PCA) - It cut the number of interdependent variables to a smaller set of uncorrelated components. :
Principal Component Analysis(PCA) is commonly used to reduce the dimension of data by decomposing data into a number of independent factors. 
It has many applications like simplifying model calculation by reducing the number of predicting factors. 
However, in our case here, we will just use the character of variable independence for PCA to remove the multi-collinearity issue in the model.

3) Ridge Regression - It is a technique for analyzing multiple regression data that suffer from multicollinearity.
4.Variable Transformation:
The second method is to transform some of the variables to make them less correlated but still maintain their feature. 
What do I mean by this? In the housing model example, I can transfer ‘years of built’ to ‘age of the house’ 
by subtracting current year by years of built. 
For example, if the year of building is1994, then the age of the house is 2020–1994=26 years.

https://towardsdatascience.com/multi-collinearity-in-regression-fe7a2c1467ea
When is multicollinearity not a problem?
1) If your goal is to predict Y from a set of X variables, then multicollinearity is not a problem. 
The predictions will still be accurate, and the overall R2 (or adjusted R2) quantifies how well the model predicts the Y values.
2) Multiple dummy (binary) variables that represent a categorical variable with three or more categories.
------------------------------------------------------------------------------------------------------------
VIF:
The Variance Inflation Factor (VIF) is used to detect multicollinearity in a multiple regression analysis.


The threshold of 5 or 10 for VIF values is a common guideline used to identify problematic multicollinearity:

VIF < 5: If the VIF value is less than 5, it generally indicates that there is low multicollinearity among the variables. 
This suggests that the variance of the coefficient estimates is not significantly inflated due to correlation between variables.

VIF > 10: If the VIF value exceeds 10, it is often considered a sign of high multicollinearity. 
This suggests that the variance of the coefficient estimates is significantly inflated due to correlation between variables, 
making the interpretation of individual coefficients less reliable.

The "variance of the coefficient estimates" is like a measure of how uncertain or shaky these estimates are. 
When the VIF (Variance Inflation Factor) is low (below 5), it suggests that the estimates of how much each factor matters (the coefficients) 
are relatively stable and not exaggerated due to the similarity between factors.

when VIF is low, it means that the relationship between each factor and the outcome is clear and not being messed up by the fact that some factors are very similar. 
It's like having a clearer picture of which factor is truly influencing the outcome.
 
----------------------------------------------------------------------------------------------------------
Support Vector Machine(SVM):

Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. 
However,  it is mostly used in classification problems. 
The main of SVM is to find a hyperplane or a boundary line that can accurately classify two classes.
It Selects the best hyperplane which has the maximum distance from the nearest datapoints of the two classes. 
Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane.
svm apart from creating the hyperplane also creates two marginal lines which makes it easy to linearly seperable for both the classes.
suppose there are two classes lets say + and - 
the margin line for + classes will pass through the nearest point from the hyperplane and the margin line will be parallel to the hyperplane,
same with the -ve class. the distance between the line and hyper plane is called margin.
In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) 
with the value of each feature being the value of a particular coordinate. 
Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.

SVM can handle both linear and non-linear separations. For non-linear cases, it uses techniques like the "kernel trick" to transform the data into higher dimensions, 
making it easier to find a separating hyperplane. 
SVM aims to create a decision boundary that not only classifies training data accurately but also generalizes well to new, unseen data.

 
Intuitively the best line is the line that is far away from both classes (has the largest margin). 
To have optimal solution, we have to maximize the margin in both ways 
(if we have multiple classes, then we have to maximize it considering each of the classes).

Using these support vectors, we maximize the margin of the classifier. 
Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.

to identify the right hyper-plane: “Select the hyper-plane which segregates the two classes better”.

maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. 
This distance is called as Margin. The hyper plane with the maximum distance between its nearest data point in each of the classes is the best
hyperplane dividing the classes.
----------------------------------------------------------------------------------------------------------------------------------------------
suppose you may have two hyperplanes one with higher margin from the datapoints but misclassified few data points and there is another hyperplane 
that is having less margin distance from the nearest data points which one does SVM select?
 svm selects the one which correctly classifies the datapoints rather than the one with
highest marginal distance.

we have to select the one with higher margin eventhough few points may be misclassified because it is an optimal value with less risk of overfitting.
----------------------------------------------------------------------------------------------------------------------------
What is Hard margin and Soft margin?

Hard Margin:
Hard margin SVM aims to find a hyperplane that perfectly separates the data points of different classes. 
In other words, no data point from either class is allowed to be inside the margin or on the wrong side of the hyperplane.
This approach works well when the data is clearly linearly separable, and there are no outliers or errors in the dataset.
However, hard margin SVM can be very sensitive to outliers or noisy data points. 
If just one point is misclassified or far from the others, it might completely change the hyperplane.

Soft Margin:
Soft margin SVM is a more flexible approach that allows for some data points to be inside the margin or even on the wrong side of the hyperplane.
This approach is useful when the data is not perfectly separable due to noise or outliers. 
Allowing some "slack" in the margin or misclassifications can lead to a more robust model.
Soft margin SVM introduces a penalty parameter (C) that balances between maximizing the margin and minimizing the classification error. 
A smaller C allows more margin violations, while a larger C enforces  a stricter boundary.
-----------------------------------------------------------------------------------------------------------
why svm has less chance of overfitting?
SVM includes a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing the classification error. 
A smaller C value increases the margin but allows more misclassifications, making the model less prone to overfitting. 
In contrast, larger C values can lead to a more accurate fit to the training data but might result in overfitting.

SVM aims to find the hyperplane that maximizes the margin between different classes while correctly classifying the training data. 
This focus on maximizing the margin helps the model generalize better to new, unseen data. 
A larger margin indicates more confidence in the model's predictions, which reduces the risk of fitting noise in the training data.

When dealing with non-linearly separable data, SVM uses the kernel trick to transform the data into a higher-dimensional space where it becomes linearly separable. 
This transformation can reduce the risk of overfitting by making the separation more principled and avoiding complex curves that can lead to overfitting.
-----------------------------------------------------------------------------------------------------------
difference between SVM and Logistic regression?
LR works with already identified  independent variable. SVM works well with unstructured and semi-structured data like text and images.
LR is vulnerable to overfitting.	The risk of overfitting is less in SVM.
LR is not used to find the best margin, instead, it can have different decision boundaries with different weights that are near the optimal point.
SVM tries to find the “best” margin (distance between the line and the support vectors) that separates the classes and thus reduces the risk of error on the data.
--------------------------------------------------------------------------------
How SVM is different from other algorithms?
Usually a learning algorithm tries to learn the most common characteristics (what differentiates one class from another) of a class and 
the classification is based on those representative characteristics learnt (so classification is based on differences between classes). 
The SVM works in the other way around. It finds the most similar examples between classes. Those will be the support vectors.
As an example, lets consider two classes, apples and lemons.
Other algorithms will learn the most evident, most representative characteristics of apples and lemons, like apples are green and rounded 
while lemons are yellow and have elliptic form.
In contrast, SVM will search for apples that are very similar to lemons, for example apples which are yellow and have elliptic form. 
This will be a support vector. The other support vector will be a lemon similar to an apple (green and rounded). 
So other algorithms learns the differences while SVM learns similarities.


------------------------------------------------------------------------------------------------------------------------------
how SVM handles outliers?
The Aim of SVM is to find a model that generalize well and perform well on the unseen data but not to perfectly fit the training data
For non-linearly separable data, SVM can use a kernel function to map the data into a higher-dimensional space where it might become linearly separable. 
Outliers that are far from the main cluster in the original space might find themselves better separated in the transformed space, reducing their influence on the decision boundary.


---------------------------------------------------------------------------------------------------------------------------
What is the advantage of performing dimensionality reduction before fitting an SVM?

Support Vector Machine Learning Algorithm performs better in the reduced space. It is beneficial to perform dimensionality reduction 
before fitting an SVM if the number of features is large when compared to the number of observations.
----------------------------------------------------------------------------------------------------------------------------------
when the data is linearly seperable SVM works easily but what when the data is non seperable?

If the data is non linearly separable  then SVM makes use of kernel tricks to make it linearly separable. 
The concept of transformation of non-linearly separable data into linearly separable is called Cover’s theorem - 
“given a set of training data that is not linearly separable, with high probability it can be transformed into a linearly separable training 
set by projecting it into a higher-dimensional space via some non-linear transformation”. 
Kernel tricks help in projecting data points to the higher dimensional space by which they became relatively more easily separable 
in higher-dimensional space.
Simply put, it does some extremely complex data transformations, then finds out the process to separate the data based on the labels or 
outputs you’ve defined.


The basic idea is that when a data set is inseparable in the current dimensions, add another dimension, maybe that way the data will be separable. 
Just think about it, the example above is in 2D and it is inseparable, but maybe in 3D there is a gap between the apples and the lemons, 
maybe there is a level difference, so lemons are on level one and lemons are on level two. In this case we can easily draw a separating hyperplane 
(in 3D a hyperplane is a plane) between level 1 and 2.

Kernel Tricks: 


The primary use of kernels is to extend SVM's capabilities to deal with data that cannot be separated using a simple linear boundary in the original feature space. 
Kernels allow SVM to capture complex relationships between features without explicitly computing the coordinates of data points in the higher-dimensional space.

Here are some common kernels and their explanations:
Kernel tricks also known as Generalized dot product. Kernel tricks are the way of calculating dot product of two vectors to 
check how much they make an effect on each other. According to Cover’s theorem the chances of linearly non-separable data sets becoming 
linearly separable increase in higher dimensions. 
Kernel functions are used to get the dot products to solve SVM constrained optimization.

1.LINEAR KERNAL
2.RBF KERNAL
3.SIGMOID KERNAL
4.POLY SVM KERNAL

linear kernel:
Formula: K(x, y) = x^T * y
When to Use: Use the linear kernel when your data is already reasonably well separated by a linear boundary.

Gaussian kernel:
F(x1,x2)=e**((x1-x2)/2sigma sqr)**d

RBF KERNAL:
RBF kernels are the most generalized form of kernelization and is one of the most widely used kernels due to its similarity 
to the Gaussian distribution. The RBF kernel function for two points X1 and X2 computes the similarity or how close they are to each other. 
This kernel can be mathematically represented as follows:
F(x1,x2)=e**(-(|x1|-|x2|))**d
where,
1. ‘σ’ is the variance and our hyperparameter
2. ||X1 - X2|| is the Euclidean (L₂-norm) Distance between two points X1 and X2
3. d is no. of dimensions

The maximum value that the RBF kernel can be is 1 and occurs when d12(distance btw x1,x2) is 0 which is when the points are the same, i.e. X1 = X2.
When the points are the same, there is no distance between them and therefore they are extremely similar
When the points are separated by a large distance, then the kernel value is less than 1 and close to 0 
which would mean that the points are dissimilar.

When to Use: RBF kernel is very versatile and works well for most cases. Use it when there is no prior knowledge about the data's underlying distribution.

https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a

3.SIGMOID KERNAL
F(x1,x2)=tanh(gamma.(x1T.x2)+r)
When to Use: Sigmoid kernel can be useful when the data has a sigmoid-like pattern, but it's less commonly used compared to linear, polynomial, and RBF kernels.
4.POLY SVM KERNAL
If you think that something can be solved using polynomial equation then we use POLY SVM KERNAL.
a polynomial relationship means that when you look at some data on a graph, the points don't make a straight line; instead, they form a curve. 
This curve can bend and change in various ways.
F(x1,x2)=(x1T.x2+1)
-------------------------------------------------------------------------------------------------------------------
when to use each kernel in SVM?

If your data is linearly separable, without a second thought, go for a linear kernel. 

Because a linear kernel takes less training time when compared to other kernel functions.

The linear kernel is mostly preferred for text classification problems as it performs well for large datasets. 
Gaussian kernels tend to give good results when there is no additional information regarding data that is available.
Rbf kernel is also a kind of Gaussian kernel which projects the high dimensional data and then searches a linear separation for it.
Polynomial kernels give good results for problems where all the training data is normalized. 

-------------------------------------------------------------------------------------------------------

Multiclass Classification Using SVM
In its most simple type, SVM doesn’t support multiclass classification natively. 

It supports binary classification and separating data points into two classes. 
For multiclass classification, the same principle is utilized after breaking down the multiclassification problem into 
multiple binary classification problems.

The idea is to map data points to high dimensional space to gain mutual linear separation between every two classes. 
This is called a One-to-One approach, which breaks down the multiclass problem into multiple binary classification problems. 
A binary classifier per each pair of classes.

Another approach one can use is One-to-Rest. In that approach, the breakdown is set to a binary classifier per each class.


In the One-to-Rest approach, the classifier can use \pmb{m} SVMs. Each SVM would predict membership in one of the \pmb{m} classes.
In the One-to-One approach, the classifier can use \pmb{\frac{m (m-1)}{2}} SVMs.

https://www.baeldung.com/cs/svm-multiclass-classification
-----------------------------------------------------------------------------------------------------------------------------------
SUPPORT Vector REGRESSION:

support vector regressor has a hyperplane with two marginal lines 
Our objective, when we are moving on with SVR, is to basically consider the hyperplane that usually has majority of the datapoints within
the marginal lines.


--------------------------------------------------------------------------------------------------------------------------------------
kernals in SVM?

Gaussian Kernel: It is used to perform transformation, when there is no prior knowledge about data.
Gaussian Kernel Radial Basis Function (RBF) : Same as above kernel function, adding radial basis method to improve the transformation.
Sigmoid Kernel: this function is equivalent to a two-layer, perceptron model of neural network, which is used as activation function for artificial neurons.
Polynomial Kernel: It represents  the similarity of vectors in training set of data in a feature space over polynomials of the original variables used in kernel.
Linear Kernel: used when data is linearly separable.
--------------------------------------------------------------------------------------------------------------------------------------
Our best fit line is the hyperplane that has a maximum number of points.
1. What Are the Basic Assumption?
There are no such assumptions

2. Advantages
SVM is more effective in high dimensional spaces.
Does not get influenced by Outliers. 
SVM is relatively memory efficient.
SVM’s are very good when we have no idea on the data.
Works well with even unstructured and semi structured data like text, Images and trees.
The kernel trick is real strength of SVM. With an appropriate kernel function, we can solve any complex problem.
SVM models have generalization in practice, the risk of over-fitting is less in SVM.

3. Disadvantages
More Training Time is required for larger dataset
It is difficult to choose a good kernel function.


5. Impact of Missing Values?
Although SVMs are an attractive option when constructing a classifier, 
SVMs do not easily accommodate missing covariate information. Similar to other prediction and classification methods, 
in-attention to missing data when constructing an SVM can impact the accuracy and utility of the resulting classifier.


--------------------------------------------------------------------------------------------------------------------------------------
what is soft margin in SVM?

SVM algorithm attempts to find a hyperplane that separates these two classes with the highest possible margin. 
If classes are fully linearly separable, a hard-margin can be used. Otherwise, it requires a soft-margin.

This idea is based on a simple premise: allow SVM to make a certain number of mistakes and keep margin as wide as possible 
so that other points can still be classified correctly. 
This can be done simply by modifying the objective of SVM.

In rare cases where the data is linearly separable, we might not want to choose a decision boundary 
that perfectly separates the data to avoid overfitting. 

Here the red decision boundary perfectly separates all the training points. 
However, is it really a good idea of having a decision boundary with such less margin? 
Do you think such kind of decision boundary will generalize well on unseen data? 
The answer is: No. The green decision boundary has a wider margin that would allow it to generalize well on unseen data. 
In that sense, soft margin formulation would also help in avoiding the overfitting problem.

How it Works (mathematically)?
Let us see how we can modify our objective to achieve the desired behavior. In this new setting, we would aim to minimize the following objective:

L=(1/2)(||W||)square+C

 Here, C is a hyperparameter that decides the trade-off between maximizing the margin and minimizing the mistakes. 
 When C is small, classification mistakes are given less importance and focus is more on maximizing the margin, 
 whereas when C is large, the focus is more on avoiding misclassification at the expense of keeping the margin small.
 
 https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe

-----------------------------------------------------------------------------------------------------

when should we use SVM?
SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems.
 SVMs tend to be unusable beyond 10 000 data points.

when should we use linear SVM?
SVM works well with unstructured and semi-structured data like text and images.
Logistic regression and SVM with a linear kernel have similar performance but depending on your features, one may be more efficient than the other.
They work well when the no of feature are more than the no of samples.

when you want to a model to predict continous output OR decrete output and the data is linearly seperable.
when you want the results to be fast irrespective of the accuracy.


when to use a kernel SVM?
when you have high dimensional data
when you want the results to be accurate irrespective of the time taken to train.
If the number of observations is larger than 50k, speed could be an issue when using the Gaussian kernel; hence, one might want to use the linear kernel.
Use the linear kernel when the number of features is larger than the number of observations.

n = number of features,
m = number of training examples
If n is large (1–10,000) and m is small (10–1000) : use logistic regression or SVM with a linear kernel.
2. If n is small (1–10 00) and m is intermediate (10–10,000) : use SVM with (Gaussian, polynomial etc) kernel
3. If n is small (1–10 00), m is large (50,000–1,000,000+): first, manually add more features and then use logistic regression or SVM with a linear kernel.

Why there is a less chance of overfitting in SVM?
In SVM, to avoid overfitting, we choose a Soft Margin, instead of a Hard one i.e. 
we let some data points enter our margin intentionally (but we still penalize it) so that our classifier don't overfit on our training sample.  
The higher the gamma, the higher the hyperplane tries to match the training data.

Which techniques are often applied to reduce overfitting in an SVM classifier?
SVM minimizes the overfit by adding structural constraints on the discriminant surface (max margin). 

Which is better KNN or SVM?
SVM take cares of outliers better than KNN. If training data is much larger than no. of features(m>>n), 
KNN is better than SVM. SVM outperforms KNN when there are large features and lesser training data.

SVM VS Random Forest?
It really depends what you want to achieve, what your data look like and etc. 
SVM will generally perform better on linear dependencies, otherwise you need nonlinear kernel and choice of kernel may change results. 
Also, SVM are less interpretable - for e.g if you want to explain why the classification was like it was - it will be non-trivial. 
Decision trees have better interpretability, they work faster and if you have categorical/numerical variables its fine, 
moreover: non-linear dependencies are handled well (given N large enough). 
Also they train faster than SVM in general, but they have tendency to overfit..
------------------------------------------------------------------------------------------------------------------

when should you use KNN?
we shouldn't use KNN for text data.
you can use the KNN algorithm for applications that require high accuracy but that do not require a human-readable model.
it is used when the data set is not easily separable using the decision planes.
it is advised to use the KNN algorithm for multiclass classification if the number of samples of the data is less than 50,000. 
Another limitation is the feature importance is not possible for the KNN algorithm. 
we shouldn't use KNN when there are outliers in the dataset as KNN cannot handle outliers well.

---------------------------------------------------------------------------------------------------------------------------
KNN vs Naive Bayes?
Naive Bayes is a linear classifier while K-NN is not; It tends to be faster when applied to big data. 
In comparison, k-nn is usually slower for large amounts of data, because of the calculations required for each new step in the process.
In general, Naive Bayes is highly accurate when applied to big data.
KNN is a discriminative algorithm since it models the conditional probability of a sample belonging to a given class.

KNN vs SVM?
SVM is less computationally demanding than kNN and is easier to interpret but can identify only a limited set of patterns. 
On the other hand, kNN can find very complex patterns but its output is more challenging to interpret
-----------------------------------------------------------------------------------------------------------------------
Advantages of KNN?
It stores the training dataset and learns from it only at the time of making real time predictions. 
This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc

Disadvantage of KNN?
Imbalanced data causes problems: k-NN doesn't perform well on imbalanced data. 
If we consider two classes, A and B, and the majority of the training data is labeled as A, then the model will ultimately give a lot of preference to A. 
This might result in getting the less common class B wrongly classified.

-----------------------------------------------------------------------------------------------------------------
Explain Decision Tree?

A decision tree is a type of supervised learning algorithm that can be used in classification as well as regressor problems. 
The input to a decision tree can be both continuous as well as categorical. The decision tree works on an if-else statement. 

lets take an example whether a person will go for a hike based on various factors such as if it is raining,what is the temperature, distance etc..

the first step is to determine first variable to classify or split the dataset, this can be done using various methods such as Gini impurity or information Gain.
The main aim of these methods is to find an optmial split such that we can get to the output ASAP. 
 lets say raining as the first variable, if it is raining we have "yes" branch, if not we have "no" branch.
if it is raining then you move to the next question: "Is the temperature above a certain level?"
If the temperature is high enough, you might decide to go for a hike despite the rain. In this case, you'd follow the "Yes" branch.
If the temperature is not high enough, you might choose not to go for a hike. In this case, you'd follow the "No" branch.
If the answer to the first question is "No" (it's not raining), you've already made your decision to go for a hike.
At the end of each branch, you'll reach a leaf node, which represents the final decision or outcome.q 
 
It is a tree structured classifier where the nodes represent the features, branches represent the rules and the leaf node represents the outcome.

Root Node: 
The first split which decides the entire population or sample data should further get divided into two or more homogeneous sets. 
 

Splitting:
It is a process of dividing a node into two or more sub-nodes. 

There are two nodes in decision tree:
1.Decision node which are used to make decisions and have multiple branches.
2.leaf nodes are the output of these decisions.

Decision tree tries to solve a problem by using tree representation(Node and Leaf)



steps in decision tree classifier:

1. Take each feature and calculate the impurity either using information gain, entropy or gini impurity.
2. if using IG consider the feature with high information gain, if using gini impurity consider the feature with low gini impurity value as a splitter.
3. repeat the above steps for all the internal nodes.

https://www.saedsayad.com/decision_tree.htm#:~:text=b)%20Entropy%20using%20the%20frequency,%2C%20the%20most%20homogeneous%20branches).
https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html

steps in decision tree regressor: 
same as above but the impurity factor used is standard deviation reduction
lower is the standard deviation value higher is the probability of getting selected as the root node.

https://www.saedsayad.com/decision_tree_reg.htm#:~:text=The%20ID3%20algorithm%20can%20be,Gain%20with%20Standard%20Deviation%20Reduction.&text=A%20decision%20tree%20is%20built,with%20similar%20values%20(homogenous).


------------------------------------------------------------------------------------------------------------------------------------------------
There are 4 popular types of decision tree algorithms: 
ID3, CART (Classification and Regression Trees), Chi-Square and Reduction in Variance.

ID3 (Iterative Dichotomiser)
ID3 decision tree algorithm uses Information Gain to decide the splitting points. 
In order to measure how much information we gain, we can use entropy to calculate the homogeneity of a sample.
Next step is to make splits that minimize entropy. We use information gain to determine the best split.


CART (Classification and Regression Tree)
Another decision tree algorithm CART uses the Gini method to create split points including Gini Index (Gini Impurity) and Gini Gain.
After calculating Gini Gain for every attribute, sklearn.tree.DecisionTreeClassifier will choose attribute with the largest Gini Gain as the Root Node. 
A branch with Gini of 0 is a leaf node while a branch with Gini more than 0 needs further splitting. 
Nodes are grown recursively until all data is classified.
As mentioned, CART can also handle regression problem using a different splitting criterion: Mean Squared Error (MSE) to determine the splitting points. 

https://www.vebuso.com/2020/01/decision-tree-intuition-from-concept-to-application/
----------------------------------------------------------------------------------------------------------------------------------------------
while constructing a decision tree there is a concept call ID3 algorithm which says that the first step is to select a right attribute or a feature
in order to split the given dataset into the form of a tree. 
we can do that splitting or on what basis we need to split is done with the help of entropy,Information gain,gini impurity.

suppose lets take we have 3 features f1,f2,f3 and output.
if we consider f1 as the root node and split the data based on the categories in the f1.
After splitting based on F1 categories we use F2 and F3 in the upcoming nodes.

The main aim of selecting the optimal root nodes is to get to the output as quickly as possible and also reduce the complexity of a tree.

what the IG,entropy,GI does is take each feature and calculate the respective IG value for the splits at each and every node.
THis process will continue splitting until we get suppose for binary classification the last leaf nodes should lets consider two leaf nodes
the each node should consist of either all zeros or all ones.

suppose while splitting only at one point we may get entropy as zero at that point we stop further splitting.
with the help of Entropy,IG,GI we try to find optimal features to split which will get to the output with minimal splits and within less time.

what happens is we initially we take a feature and split it and again at the next node we take another feature and calculate the entropy 
we usually sum up all the entropies to caluculate the aggregated entropy in order to consider all those entropies we use Information Gain.



---------------------------------------------------------------------------------------------------------------------
Information Gain and Entropy:
https://www.youtube.com/watch?v=1IQOtJ4NI_0
What is entropy and what basically it does?
Entropy calculates the measure of randomness within the dataset.
For example lets consider a box of balls, if the box contains balls with various colors then entropy is high, else it is low.
Entropy usually ranges between 0 and 1. it can be greater than 1 but it means the same thing , a very high level of disorder. 

In decision trees the main of of entropy is to find a feature which splits the data into subsets and each subset has less mixed up values.
feature with less entropy will be taken as an optimal feature to split.
The higher the entropy, the harder it is to draw any conclusions from that information. 
 

entropy  E(S)=summation i=0 to n(-pi*log2 pi)
Where ‘Pi’ is simply the frequentist probability of an element/class ‘i’ in our data.
 
For simplicity’s sake let’s say we only have two classes , a positive class and a negative class. 
Therefore ‘i’ here could be either + or (-). 
So if we had a total of 100 data points in our dataset with 30 belonging to the positive class and 70 belonging to the negative class 
then ‘P+’ would be 3/10 and ‘P-’ would be 7/10. Pretty straightforward.
If I was to calculate the entropy of my classes in this example using the formula above. Here’s what I would get.

Entropy=-(3/10) log2(3/10)-(7/10) log2(7/10)7==0.88

The entropy here is approximately 0.88. This is considered a high entropy , a high level of disorder ( meaning low level of purity). 
Entropy is measured between 0 and 1.(Depending on the number of classes in your dataset, entropy can be greater than 1 but it means the same thing , 
a very high level of disorder. 
------------------------------------------------------------------------------------------------------------
What is Information gain and why it is matter in Decision Tree, is entropy alone not enough for splitting?
Entropy gives us a general idea of the disorder in the dataset, but it doesn't guide us toward the best feature to split on.
Information gain allows us to rank features based on their effectiveness in reducing uncertainty.
This helps us focus on features that significantly contribute to making accurate predictions.

Now we know how to measure disorder. Next we need a metric to measure the reduction of this disorder.
with the help of information gain we will be able to know which feature is good at each node given a root node.
in our target variable/class given additional information( features/independent variables) about it. 
This is where Information Gain comes in. Mathematically it can be written as:
Definition: Information gain (IG) measures how much “information” a feature gives us about the class.


Steps in Information Gain:

1.Initial Entropy:
Entropy is calculated for the entire dataset before any splits. High entropy indicates that the data is mixed and disordered in terms of class distribution.

2.Calculating Entropy after Split:

For each feature, the decision tree calculates the entropy of the dataset after splitting it based on that feature.
Entropy after the split represents the degree of disorder or randomness in the subsets formed by the split.

3.Information Gain Calculation:

Information gain is the difference between the initial entropy and the average entropy after the split over all possible outcomes (branches) of the split.

Mathematically, it's calculated as:
Information Gain = Initial Entropy - Average Entropy After Split

Higher information gain indicates a more organized split that reduces uncertainty in class labels.


4.Choosing the Best Split:

The decision tree selects the feature that results in the highest information gain.
A high information gain implies that the feature's split helps in making the classes more distinct and organized.

5.Recursive Process:

After selecting the best feature for the root node, the process is repeated for the child nodes.
The goal is to continue splitting features to reduce entropy and make the decision tree more accurate.

Why it matter ?
Information gain is the main key that is used by Decision Tree Algorithms to construct a Decision Tree.
Decision Trees algorithm will always tries to maximize Information gain.
An attribute with highest Information gain will tested/split first.
The information gain is based on the decrease in entropy after a dataset is split on an attribute.

steps for deciding which feature to be used to split using entropy and information gain.

1. calculate the entropy of the target.
2. calculate the entropy of each  target given feature
3. calculate the information gain of target given a feature=entropy of target- entropy of target given feature.

https://www.saedsayad.com/decision_tree.htm#:~:text=b)%20Entropy%20using%20the%20frequency,%2C%20the%20most%20homogeneous%20branches).
https://medium.com/coinmonks/what-is-entropy-and-why-information-gain-is-matter-4e85d46d2f01

------------------------------------------------------------------------------------------------

Gini Impurity:
Gini impurity tells us how "impure" the subsets created by a split are in a decision tree. 
When you split the data based on a feature, you create subsets. Each subset will have its own class labels. 
Gini impurity now tells you how mixed up or impure these subsets are separately.

It is calculated by subtracting the sum of squared probabilities of each class from one. 
It favors larger partitions and easy to implement whereas information gain favors smaller partitions with distinct values.

The Gini impurity measure is one of the methods used in decision tree algorithms to decide the optimal split from a root node, and subsequent splits.
Def: Gini Impurity tells us what is the probability of misclassifying an observation.
For calculating the Gini impurity of a continous feature we take a threshold value to split the data.
Note that the lower the Gini the better the split. In other words the lower the likelihood of misclassification.
lesser the value of Gini Impurity, more is the probability of using the feature as the root node or internal node.

formula
Suppose we have a list of observations, that indicates if a person decided to stay home from work. 
We also have two features, namely if they are sick and their temperature.
We need to choose which feature, emotion or temperature, to split the data on. A Gini Impurity measure will help us make this decision.

Formula:
probability of all classes=p1*(2*PL1*PL2)+P2*(2*Pr*Pr2)
Gini=1-probability of sum of square of all the classes
2 because there are two groups of data.
p1 is probabiliity of sick
p2 is probabiliity of no sick
PL1 is probability of sick who stay at home
PL2 is probability of sick who doesn't stay at home
Pr1 is probability of nosick who stay at home
Pr2 is probability of nosick who doesn't stay at home

p1=no of sick people/total no of people
p2=no of no sick people/total no of people

https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/
https://towardsdatascience.com/gini-impurity-measure-dbd3878ead33#:~:text=Introduction,root%20node%2C%20and%20subsequent%20splits.&text=Def%3A%20Gini%20Impurity%20tells%20us,Gini%20the%20better%20the%20split.

Example2:
Calculating Gini Impurity:

Calculate Probabilities:

For the entire dataset, there are 4 successful treks (Yes) and 2 unsuccessful treks (No).
Probability of success (p(Yes)) = 4 / 6 ≈ 0.67
Probability of failure (p(No)) = 2 / 6 ≈ 0.33
Square Probabilities:

Square the probabilities: p(Yes)² ≈ 0.45, p(No)² ≈ 0.11
Sum of Squares:

Add the squared probabilities: 0.45 + 0.11 = 0.56
Calculate Gini Impurity:

Subtract the sum of squared probabilities from 1: Gini Impurity = 1 - 0.56 = 0.44

-------------------------------------------------------------------------------------------------------

when to use gini impurity?
mostly all consider gini impurity as a splitting parameter because IG is cost effective as in  entropy we need to calculate log values.
GI is faster than IG.
Gini impurity is computationally cost less and takes less time to execute for that reason we can go with gini impurity.
----------------------------------------------------------------------------------------------------
the gini criterion is much faster because it is less computationally expensive. 
On the other hand, the obtained results using the entropy criterion are slightly better.
entropy definitely has an edge in some data cases involving high imbalance.
ID3 works well for binary classfication and gini works well during multiclass classification.
-------------------------------------------------------------------------------------------------------

How to control leaf height and Pruning?
Answer:
To control the leaf size, we can set the parameters:-
1. Maximum depth :
Maximum tree depth is a limit to stop the further splitting of nodes when the specified tree depth has been reached during the building 
of the initial decision tree.
NEVER use maximum depth to limit the further splitting of nodes. In other words: use the largest possible value.
2. Minimum split size:
Minimum split size is a limit to stop the further splitting of nodes when the number of observations in the node is lower than the minimum split size.
This is a good way to limit the growth of the tree. When a leaf contains too few observations, further splitting will result in overfitting (modeling of noise in the data).
3. Minimum leaf size
Minimum leaf size is a limit to split a node when the number of observations in one of the child nodes is lower than the minimum leaf size.
Pruning is mostly done to reduce the chances of overfitting the tree to the training data and reduce the overall complexity of the tree.

---------------------------------------------------------------------------
There are two types of pruning: Pre-pruning and Post-pruning.
1. Pre-pruning is also known as the early stopping criteria. As the name suggests, the criteria are set as parameter values while building the model. 
The tree stops growing when it meets any of these pre-pruning criteria, or it discovers the pure classes.

2. In Post-pruning, the idea is to allow the decision tree to grow fully and observe the CP value. 
Next, we prune/cut the tree with the optimal CP(Complexity Parameter) value as the parameter.


-----------------------------------------------------------------------------------------------
why decision trees are prone to overfitting and how can we handle them?

when we give a dataset to a decision tree, it starts splitting irrespective of whether the variables have an impact or not.
For example i have a dataset to predict the fruit name and i have variable which says lines(if lines are there or not on the fruit) this might not be relevant 
but decision tree considers it while splitting.
decision tree tries to split the data based on each feature, and only tries to fit  the dataset, but doesn't try to generalize because of which it doensn't perform well
on unseen data.

if the no. of variables are more then it creates a more complex model, which will have high variance and results on unseen data is not accurate.
Decision trees are prone to overfitting, especially when a tree is particularly deep.

Especially when your dataset has many features(high dimension), it tends to overfit more
 
How to reduce overfitting in decision tree?

One of the methods used to address over-fitting in decision tree is called pruning which is done after the initial training is complete. 
In pruning, you trim off the branches of the tree, i.e., remove the decision nodes starting from the leaf node such that the overall accuracy is not disturbed. 
This is done by segregating the actual training set into two sets: training data set, D and validation data set, V. 
Prepare the decision tree using the segregated training data set, D. 
Then continue trimming the tree accordingly to optimize the accuracy of the validation data set, V.


Decision tree tends to overfit since at each node, it will make the decision among a subset of all the features(columns), so when it reaches a final decision, 
it is a complicated and long decision chain. Only if a data point satisfies all the rules along this chain, the final decision can be made. 
This kind of specific rules on training dataset make it very specific for the training set, on the other hand, cannot generalize well for new data points that it has never seen. 
.
There are parameters to control how deep your tree will grow, how many leaf node your tree will have when you grow a tree and so on. 
These parameters can give you the chance to prevent your tree from overfitting. 
However, in general, decision tree tends to grow deep to make a decision which makes it specific and complicated, as a result tends to overfit.
There are also concepts like 'unstable' and 'high variance' to describe this characteristic for tree models. 'High variance' and 'unstable' mean that the model is easy to change, 
say, your model could be completely different if the input changes a little bit.
If a model is too easy to change according to the input data, too flexible, 'high variance' , can fit every input dataset too well, 
makes it hard to generalize a bigger idea about the data trend. This also explains why overfit.

-------------------------------------------------------------------------------------------------
How decision tree handles missing values?

There are several methods used by various decision trees. Simply ignoring the missing values (like ID3 and other old algorithms does) or 
treating the missing values as another category (in case of a nominal feature) are not real handling missing values. 
However those approaches were used in the early stages of decision tree development.

The real handling approaches to missing data does not use data point with missing values in the evaluation of a split. 
However, when child nodes are created and trained, those instances are distributed somehow.

I know about the following approaches to distribute the missing value instances to child nodes:

all goes to the node which already has the biggest number of instances (CART, is not the primary rule)
distribute to all children, but with diminished weights, proportional with the number of instances from each child node (C45 and others)
distribute randomly to only one single child node, eventually according with a categorical distribution (I have seen that in various implementations of C45 and CART 
for a faster running time) build, sort and use surrogates to distribute instances to a child node, where surrogates are input features
which resembles best how the test feature send data instances to left or right child node (CART, if that fails, the majority rule is used)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1. What Are the Basic Assumption?
There are no such assumptions.

2. Advantages
Advantages of Decision Tree

Clear Visualization: 
The algorithm is simple to understand, interpret and visualize as the idea is mostly used in our daily lives. 
Output of a Decision Tree can be easily interpreted by humans.

Simple and easy to understand: Decision Tree looks like simple if-else statements which are very easy to understand.

Decision Tree can be used for both classification and regression problems.

Decision Tree can handle both continuous and categorical variables.

No feature scaling required: 
No feature scaling (standardization and normalization) required in case of Decision Tree as it uses rule based approach instead of distance calculation.

Handles non-linear parameters efficiently: 
Non linear parameters don't affect the performance of a Decision Tree unlike curve based algorithms. 
So, if there is high non-linearity between the independent variables, Decision Trees may outperform as compared to other curve based algorithms.

Decision Tree can automatically handle missing values.

Decision Tree is usually robust to outliers and can handle them automatically.

Less Training Period: Training period is less as compared to Random Forest because it generates only one tree unlike forest of trees in the Random Forest.

3. Disadvantages
Disadvantages of Decision Tree

Overfitting: 
This is the main problem of the Decision Tree. 
It generally leads to overfitting of the data which ultimately leads to wrong predictions.
 In order to fit the data (even noisy data), it keeps generating new nodes and ultimately the tree becomes too complex to interpret. 
In this way, it loses its generalization capabilities. 
It performs very well on the trained data but starts making a lot of mistakes on the unseen data.
High variance: As mentioned in point 1, Decision Tree generally leads to the overfitting of data. 
Due to the overfitting, there are very high chances of high variance in the output which leads to many errors in the final estimation 
and shows high inaccuracy in the results. In order to achieve zero bias (overfitting), it leads to high variance.

Unstable: Adding a new data point can lead to re-generation of the overall tree and all nodes need to be recalculated and recreated.

Not suitable for large datasets: If data size is large, then one single tree may grow complex and lead to overfitting. 
So in this case, we should use Random Forest instead of a single Decision Tree.

They are weak learners: a single decision tree normally does not make great predictions, 
so multiple trees are often combined to make ‘forests’ to give birth to stronger ensemble models. 

4. Whether Feature Scaling is required?
No

6. Impact of outliers?
It is not sensitive to outliers.Since, extreme values or outliers, never cause much reduction in RSS, they are never involved in split. 
Hence, tree based methods are insensitive to outliers.

Types of Problems it can solve(Supervised)
Classification
Regression

Performance Metrics:

Classification:

Confusion Matrix
Precision,Recall, F1 score

Regression
R2,Adjusted R2
MSE,RMSE,MAE

https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052

https://www.saedsayad.com/decision_tree_reg.htm#:~:text=Decision%20tree%20builds%20regression%20or,decision%20nodes%20and%20leaf%20nodes.

---------------------------------------------------------------------------------------------------------------------------------------------------
There are two types of pruning: Pre-pruning and Post-pruning.
1. Pre-pruning is also known as the early stopping criteria. As the name suggests, the criteria are set as parameter values while building the model. 
The tree stops growing when it meets any of these pre-pruning criteria, or it discovers the pure classes.

2. In Post-pruning, the idea is to allow the decision tree to grow fully and observe the CP value. 
Next, we prune/cut the tree with the optimal CP(Complexity Parameter) value as the parameter.
--------------------------------------------------------------------------------------------------------------------------
How to handle a decision tree for numerical and categorical data?

Decision trees can handle both categorical and numerical variables at the same time as features. There is not any problem in doing that.
Every split in a decision tree is based on a feature.
1. If the feature is categorical, the split is done with the elements belonging to a particular class.
2. If the feature is continuous, the split is done with the elements higher than a threshold.
At every split, the decision tree will take the best variable at that moment. This will be done according to an impurity measure with the split branches. 
----------------------------------------------------------------------------------------------------------------

Which is the frequent technique to reduce overfitting?

One of the most powerful features to avoid/prevent overfitting is cross-validation. 
The idea behind this is to use the initial training data to generate mini train-test-splits, and then use these splits to tune your model. 
In a standard k-fold validation, the data is partitioned into k-subsets also known as folds.


when should we use Decision Tree regressor?
when you want to a model to predict continous output 
when you want the results to be fast irrespective of the accuracy and the results must be easily interpreted(understood)


when should you use random forest?
it can be used for classification problem and also for predicting the continous output.
when you want the results to be accurate irrespective of the time taken to train.
we use random forest when you want to know the feature importance.
we should try with random forest when there is noise in the data as it performs well even if there is some noise in the dataset.

Random forests and gradient boosting each excel in different areas. 
Random forests perform well for multi-class object detection and bioinformatics, which tends to have a lot of statistical noise

when should you use Gradient boosting?
it can be used for classification problem and also for predicting the continous output.
when you want the results to be accurate irrespective of the time taken to train.
we should go with gradient boosting when the dataset is having less noise.
Gradient Boosting performs well when you have unbalanced data such as in real time risk assessment.

-------------------------------------------------------------------------------------------------------------------
what are ensemble methods?
Ensemble methods involve aggregating multiple machine learning models with the aim of decreasing both bias and variance. 
Ideally, the result from an ensemble method will be better than any of individual machine learning model.
------------------------------------------------------------------------------------------------------------
what is Bagging?

bagging is an ensemble techinque which is  also called as bootstrap aggregation.
example: random forest classifier where we use multiple decision tress.
In bagging we create multiple weak learning models and aggregate the results to get the combined result

suppose there is a dataset with n number of records.
we will shuffle the data and pick few records and train a model on the datset
again we shuffle the data and pick some few records and train another model 
in the same way we do the same process and create n number of models
The picking of samples is called row sampling with replacement.
after training the models with the data we will test all the models with the same test data.
we might get same or different or same outputs for all the models.
to the output of all the models we apply voting classifier basically what it does is picks the output with majority of votes.

There are two steps in bagging:
-row sampling with replacement step where the data is splitted and given as input is known as bootstrap
In sklearn bootstrap is True for with replacement. If False, sampling without replacement is performed.
- the place where all the outputs are combined to give single output is known for aggregation
that's why the name bootstrap aggregation.


Algorithms that follow bagging technique:

Bagging algorithms:

Bagging meta-estimator
Random forest
-----------------------------------------------------------------------------------------------------------------------
What is the Random Forest Algorithm?
Answer:
Random forest is a supervised ML algorithm which works for both classificationa as well as regression problems.
Random forest is an ensembling technique which combines the output of multiple individual models to give the final output.
when dealing with a classification problem most popular answer is considered as the final answer and average of multiple decision trees is taken as the final 
answer while working on a regression problem.
Each individual model is given only a sample of data as input and at each node only a set of features are considered for splitting.
The splitting of row sampling with replacement is called bootstrap. 
The step which involves combining all the results and generating output based on majority voting is known as aggregation.
The predictions from a random forest are more accurate and reliable compared to that of an individual decision tree.


It solves the problem of :
overfitting and underfitting

Random Forest is an ensemble machine learning algorithm that follows the bagging technique. 
The base estimators in the random forest are decision trees. Random forest randomly selects a set of features 
that are used to decide the best split at each node of the decision tree.

Looking at it step-by-step, this is what a random forest model does:
1. Random Forest begins by randomly selecting subsets of the training data through a process called bootstrap sampling. 
   This involves creating multiple datasets of the same size as the original by randomly selecting instances with replacement.
2. For Each subset a decision tree is build. At each node in the decision tree, only a random set of features are considered to decide the best split.
   This is known as feature subsampling. The goal is to introduce randomness and diversity among the trees, which prevents the model from overfitting on specific features.
    The trees grow until a predefined condition, such as reaching a maximum depth or having too few instances in a node.
	These trees can have different structures due to the randomness introduced.
3. The out-of-bag (OOB) samples, which were not included in the bootstrap sample for a particular tree, are used to evaluate the model's performance. 
   OOB samples act as a validation set for each tree, providing an estimate of how well the model might generalize to unseen data.

4. The final prediction is calculated by averaging the predictions from all decision trees.

To sum up, the Random forest randomly selects data points and features and builds multiple trees (Forest).
Random Forest is used for feature importance selection. The attribute (.feature_importances_) is used to find feature importance.
Some Important Parameters:-
1. n_estimators:- It defines the number of decision trees to be created in a random forest.
2. criterion:- "Gini" or "Entropy."
3. min_samples_split:- Used to define the minimum number of samples required in a leaf node before a split is attempted
4. max_features: -It defines the maximum number of features allowed for the split in each decision tree.
5. n_jobs:- The number of jobs to run in parallel for both fit and predict. Always keep (-1) to use all the cores for parallel processing.
----------------------------------------------------------------------------------------------------------------------------
Why considering considering only a subsample of data for each model and considering only a set of features works well in random forest?

The strength of the ensemble model is diversity and this is introduced by random feature selection.
the diversity introduced by these techniques makes it less likely that all trees will make the same errors. This can lead to a more accurate and reliable ensemble model.

If each tree were trained on the full set of features and same data, they might become too similar and provide similar predictions.
By training on different subsets of data and considering different sets of features, each tree focuses on distinct features and relationships,
leading to a more comprehensive understanding of the data.
When dealing with biased or imbalanced datasets, considering only a subset of data at each node helps reduce the impact of the bias. 
It ensures that the model is exposed to different subsets of the data during training, which can lead to a more unbiased and fair prediction.
Training on a random subset of the data and features at each node allows for parallelization and faster computation. 

Overfit models tend to have high variance because they are overly complex and sensitive to small fluctuations in the training data. 
By training multiple decision trees on diverse data subsets, Random Forest reduces the variance associated with each tree.

The diversity in data subsets and feature selections makes the Random Forest ensemble robust to outliers and noisy data points. 
Individual trees may be affected by outliers, but their impact is mitigated when combined with predictions from other trees.

----------------------------------------------------------------------------------------------------------------------------------------

How Feature Importance is calculated in random Forest?
Initially, the Random Forest is trained on the original dataset, where Feature A has its genuine values, and the model achieves an accuracy of 85%.
For the evaluation with the permuted Feature A, we take the same dataset but randomly shuffle or  the values of Feature A keeping all instances same.
This means that the relationship between the original values of Feature A and the target variable is disrupted.
The Random Forest, which has been trained on the original dataset, is now evaluated on this modified dataset with permuted Feature A. 
The accuracy obtained from this evaluation is observed to be 80%.
The importance of Feature A is calculated as the difference between the original accuracy (85%) and the accuracy after permuting Feature A (80%), 
resulting in an importance score of 5%.
Repeat the process for all the features to get there importances.
normalize the importance scores to ensure that the sum across all features equals 1.

Important properties of Random Forest Classifiers
Decision Tree---Low Bias And High Variance

Ensemble Bagging(Random Forest Classifier)--Low Bias And Low Variance

1. What Are the Basic Assumption?
There are no such assumptions

2. Advantages
Advantages of Random Forest

Doesn't Overfit

Favourite algorithm for Kaggle competition

Less Parameter Tuning required

Decision Tree can handle both continuous and categorical variables.

No feature scaling required: No feature scaling (standardization and normalization) required in case of Random Forest as it uses DEcision Tree internally

Suitable for any kind of ML problems

3. Disadvantages
Disadvantages of Random Forest

1.Biased With features having many categories

Biased in multiclass classification problems towards more frequent classes.
4. Whether Feature Scaling is required?
No

6. Impact of outliers?
Robust to Outliers

Types of Problems it can solve(Supervised)
Classification
Regression


Performance Metrics
Classification
Confusion Matrix
Precision,Recall, F1 score
Regression
R2,Adjusted R2
MSE,RMSE,MAE

------------------------------------------------------------------------------------------------------------
When to use random forest?
It can be used for both classification (RandomForestClassifier)and regression (RandomForestRegressor) problems
You are interested in the significance of predictors (feature importance)
You need a quick benchmark model as random forest are quick to train and require minimal preprocessing e.g. feature scaling
If you have messy data e.g. missing data, outliers etc

This method is especially attractive for this application in the following cases:

The real-world data is noisy and contains many missing values, some of the attributes are categorical, or semi-continuous.
There are needs to integrate different data sources which face the issue of weighting them.
We need high predictive accuracy for a high-dimensional problem with highly correlated features.

When not to use random forest?
If you are solving a complex, novel problem
Transparency is important
Prediction time is important as the model needs time to aggregate the result from multiple decision trees before arriving at the final prediction

----------------------------------------------------------------------------------------------------------------

Bagging meta-estimator
Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. 
It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:

Random subsets are created from the original dataset (Bootstrapping).
The subset of the dataset includes all features.
A user-specified base estimator is fitted on each of these smaller sets.
Predictions from each model are combined to get the final result.
-------------------------------------------------------------------------------------------------------------------------
when to use gradient boosting?
It can be used for both classification (GradientBoostingClassifier) and regression (GradientBoostingRegressor) problems
You are interested in the significance of predictors (feature importance)
Prediction time is important because, unlike random forest, decision trees under gradient boosting cannot be built in parallel 
thus the process of building successive trees will take some time.

When not to use gradient boosting?
Transparency is important
Training time is important or when you have limited compute power
Your data is really noisy as gradient boosting tends to emphasise even the smallest error and as a result, it can overfit to noise in the data

-----------------------------------------------------------------------------------------------------------------
Random Forest vs GBM?

RF is much easier to tune than GBM. There are typically two parameters in RF: number of trees and number of features to be selected at each node.
RF is harder to overfit than GBM.
GBMs are more sensitive to overfitting if the data is noisy.
Training generally takes longer in GBM because of the fact that trees are built sequentially.
RFs train each tree independently, using a random sample of the data.
RF methods can handle a large amount of training data efficiently and are inherently suited for multi-class problems.
--------------------------------------------------------------------------------------------------------------------
How RF and GBM handles missing values?
A very important point regarding how RF and GBM methods are handling missing data. 
Gradient Boosting Trees uses CART trees (in a standard setup, as it was proposed by its authors). 
CART trees are also used in Random Forests. CART handles missing values either by imputation with average, either by rough average/mode, either by an averaging/mode based on proximities. 
However, one can build a GBM or RF with other types of decision trees. The usual replacement for CART is C4.5 proposed by Quinlan. In C4.5 the missing values are not replaced on data set. 
Instead, the impurity function computed takes into account the missing values by penalizing the impurity score with the ratio of missing values 

----------------------------------------------------------------------------------------------------------------
What is Naive Bayes algorithm?

It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. 
In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated 
to the presence of any other feature.
 So for predicting the output the given features are enough may be few less than the features used in the training dataset 
 which is a must in other cases.
For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. 
Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute 
to the probability that this fruit is an apple and that is why it is known as ‘Naive’.

How Naive Bayes algorithm works?
Step 1: Convert the data set into a frequency table
Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.


Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. 
The class with the highest posterior probability is the outcome of prediction.


Problem: Players will play if weather is sunny. Is this statement is correct?
We can solve it using above discussed method of posterior probability.
P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)



Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64
Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.
Naive Bayes uses a similar method to predict the probability of different class based on various attributes. 
This algorithm is mostly used in text classification and with problems having multiple classes.
Lets suppose we need to calculate whether the person will play today or not 
The input features should be provided like sunny overcast rainy by using all the probabilities from the training set we will predict the output.
-------------------------------------------------------------------------------------------------------------------------------------------
What are the Pros and Cons of Naive Bayes?
Pros:
•	It is easy and fast to predict class of test data set. It also perform well in multi class prediction
•	When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression 
	and you need less training data.
•	It perform well in case of categorical input variables compared to numerical variable(s). 
v	For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).

Cons:
•	If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.
•	On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.
•	Another limitation of Naive Bayes is the assumption of independent predictors. 
In real life, it is almost impossible that we get a set of predictors which are completely independent.

------------------------------------------------------------------------------------------------------------------------------

What is KNN Classifier ?
KNN means K-Nearest Neighbour Algorithm. It can be used for both classification and regression.
Also known as lazy learning.

KNN doesn't learn anything from the data it only keeps the training data as a reference in order to predict the values at testing time.
During the testing phase it tries to find the distance between each training datapoint with the giving testing data point.
we need to mention the no of neighbors it should take into account.
after calculating the distance between each training datapoint w.r.to testing point it will sort all the values.
suppose lets say k=5
then it will consider the top  5 sorted values and looks at each class of training datapoint.
the majority class will be taken as the class of test data point
There are many distance measures to choose from to match the structure of your input data.

How to choose the value of K: 
K value is a hyperparameter which needs to choose during the time of model building
Also, a small number of neighbors are most flexible fit, which will have a low bias, 
but the high variance and a large number of neighbors will have a smoother decision boundary, which means lower variance but higher bias.
We should choose an odd number if the number of classes is even. It is said the most common values are to be 3 & 5.

 four different distance functions, which are Euclidean distance,manhattan, cosine similarity measure, Minkowsky, correlation, and Chi square, 
 are used in the k-NN classifier respectively.
 
 How knn learns during the training?
 
 KNN stores the entire training dataset which it uses as its representation. KNN does not learn any model. 
 KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance. 
 There are many distance measures to choose from to match the structure of your input data.
 During training phase, KNN arranges the data (sort of indexing process) in order to find the closest neighbors efficiently during the inference phase. Otherwise, it would have to compare each new case during inference with the whole dataset making it quite inefficient.

---------------------------------------------------------------------------------------------------------------------
Advantages of KNN

1. No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. 
2. Since the KNN algorithm requires no training before making predictions, 
new data can be added seamlessly which will not impact the accuracy of the algorithm.

3. KNN is very easy to implement. There are only two parameters required to implement KNN i.e. 
the value of K and the distance function (e.g. Euclidean or Manhattan etc.)


Disadvantages of KNN

1. Does not work well with large dataset: 
In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of 
the algorithm.

2. Does not work well with high dimensions: 
The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, 
it becomes difficult for the algorithm to calculate the distance in each dimension.

3. Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. 
If we don't do so, KNN may generate wrong predictions.

4. Sensitive to noisy data, missing values and outliers: 
KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers.

--------------------------------------------------------------------------------------------------------------------------------

explain boosting ?

Boosting is an ensemble modeling technique which attempts to build a strong classifier from the number of weak classifiers.
It's like learning from your mistakes – you focus on the things you got wrong before and try to get them right next time. 
Each new model pays more attention to the errors made by the previous ones, so together, they become really good at predicting. 
It's like building a team where each member specializes in fixing the mistakes of the previous member, making the overall team smarter and better at making predictions.
This procedure is continued and models are added until either the complete training data set is predicted correctly or the max number of models are added.
what happens in boosting is lets suppose we have a dataset with 10 records.

Here’s how the algorithm works:

Step 1: The base algorithm reads the data and assigns equal weight to all the 10 records. 
 Equal weight means every sample has the equal probabilty to get selected into D1 which is the sample dataset for training model M1.
 The trained model is known as  the base learner.

Step 2: The base learner will now be tested with all the 10 records, False predictions made by the base learner are identified.
 In the next iteration, these false predictions are given higher weightage compared to the truely predicted samples which makes them more
probable of being selected to be trained using the next base learners.

Step 3: Repeat step 2 until the algorithm can correctly classify the output. Atlast we will combine all the weak learners to make a strong model.
	  i.e we take the output of all the weak learners and apply voting classifier on the outputs.

Therefore, the main aim of Boosting is to focus more on miss-classified predictions.

There are three boosting techniques:

1.Adaptive Boosting or AdaBoost

2.Gradient Boosting

3.XGBoost

--------------------------------------------------------------------------------------------------------------------

Adaptive boosting:

1.Adaptive Boosting or AdaBoost
AdaBoost algorithm, short for Adaptive Boosting, is a Boostin g technique that is used as an Ensemble Method in Machine Learning.
It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights to incorrectly classified instances. 
It works on the principle where learners are grown sequentially. 
Except for the first, each subsequent learner is grown from previously grown learners. 
In simple words, weak learners are converted into strong ones.
Adaboost algorithm also works on the same principle as boosting, but there is a slight difference in working.
Here learning happens by updating the weights,
weights are increased for misclassified samples and are decreased for correctly classified samples.
here the decision tree level is only 1 known as stump.

How AdaBoost Works?
First, let us discuss the working of boosting. 
It makes n number of decision trees during the training period  of data. As the first decision tree/model is made, 
the record which is incorrectly classified during the first model is given more priority. Only these records are sent as input for the second model. 
The process will go on until we specify a number of base learners we want to create. 
Remember, the repetition of records is allowed with all boosting techniques.

Step 1 – Creating First Base Learner
Now it’s time to create the first base learner. 
The algorithm takes the first feature, i.e., feature 1, and creates the first stump f1.
It will create the same number of stumps as the number of features. 
Here, it will create 3 stumps as there are only 3 features in this dataset. 
From all these stumps it will create three decision trees and can be called stumps base learner model. 
Out of these 3 models, the algorithm selects only one. 
For selecting a base learner, there are two properties, those are, Gini and Entropy. 
We must calculate Gini or Entropy the same way it is calculated for decision trees.
The stump that has the least value will be the first base learner. 
all the 3 stumps can be made with 3 features. The number below the leaves represents the correctly and incorrectly classified records. 
By using these records, the Gini or entropy index is calculated. The stump that has the least entropy or Gini will be selected for the base learner. 
Let’s assume that the entropy index is the least for stump 1. So, let’s take stump 1, i.e., feature 1 as our first base learner.

Here, feature (f1) has classified 4 records correctly and 1 incorrectly. 
The row in the figure that is marked red is incorrectly classified. For this, we will be calculating the total error.


Step 2 – Calculating the Total Error (TE)
The total error is the sum of all the errors in the classified record for sample weights. 
In our case, there is only 1 error, so Total Error (TE) = 1/5.

Step 3 – Calculating Performance of Stump
Formula for calculating Performance of Stump is: –

Performance of Stump Formula
where, ln is natural log and TE is Total Error.

In our case, TE is 1/5.
p=1/2(ln(1-TE)/TE)
By putting the value of total error in the above formula and after solving, we get the value for the performance of Stump as 0.693. 
we calculate the perfomance in order to update the weights 
for wrongly classified records, weights are updated using below formula
w=1/n*e**(perfomance)
for correctly classified records  weights are updated using below formula
w=1/n*e**(-perfomance)

i.e we increase the weights of wrongly classified samples and decrease the weights of correctly classified samples.

As is known, the total sum of all the weights should be 1. But in this case, one can see that the total updated weight of all the records is not 1, 
it’s 0.799. To make the total sum 1, one must divide every updated weight by the total sum of updated weight.
 For example, if our updated weight is 0.399 and we divide this by 0.799, i.e. 0.399/0.799=0.50. 

Step 5 – Creating New Dataset
Now, it’s time to create a new dataset from our previous one. 
In the new dataset, the frequency of incorrectly classified records will be more than the correct ones. 
While considering these normalized weights, we have to create a new dataset and that dataset is based on normalized weights. 
It will probably select the wrong records for training purposes. That will be the second decision tree/stump.
To make a new dataset based on normalized weight, the algorithm will divide it into buckets.

So, our first bucket is from 0 – 0.13, second will be from 0.13 – 0.63(0.13+0.50), third will be from 0.63 – 0.76(0.63+0.13), and so on. 
After this the algorithm will run 5 iterations to select different-different records from the older dataset. 
Suppose, in 1st iteration, the algorithm will take a random value 0.46, then it will go and see in which bucket 
that value falls and selects that records in the new dataset, 
then again it will select a random value and see in which bucket it is and select that record for the new dataset
and the same process is repeated for 5 times. 

There is a high probability for the wrong records to get selected several times. This will be the new dataset. 

How does the algorithm decide output for test data?
Suppose with the above dataset, the algorithm constructed 3 decision trees or stumps, 
the test dataset will pass through all the stumps which have been constructed by the algorithm. While passing through the 1st stump, 
it gives the output as 1, passing through 2nd stump it again gives the output as 1, and while passing through 3rd stump it gives the output as 0.
So, in AdaBoost algorithm also, the majority of votes take place between the stumps, the same as in random trees.
 And in this case, the final output will be 1. This is how the output with test data is decided.

adaboost reference
https://www.youtube.com/watch?v=NLRO1-jp5F8&t=211s
https://www.mygreatlearning.com/blog/adaboost-algorithm/#:~:text=AdaBoost%20algorithm%2C%20short%20for%20Adaptive,weights%20to%20incorrectly%20classified%20instances.


--------------------------------------------------------------------------------------------------------------

1. What Are the Basic Assumption?
There are no such assumptions

Missing Values
Adaboost can handle mising values
Xgboosst and GBoost cannot handle missing values
2. Advantages
Advantages of Adaboost

Doesn't Overfit

It has few parameters to tune

Advantages of Gradient Boost And Xgboost

It has a great performance
It can solve complex non linear functions
It is better in solve any kind of ML usecases.
3. Disadvantages
Disadvantages of Gradient Boosting And Xgboost

1.It requires some amount of parameter tuning

4. Whether Feature Scaling is required?
No

6. Impact of outliers?
Robust to Outliers in Gradient Boosting And Xgboost, Sensitive to outliers in Adaboost

Types of Problems it can solve(Supervised)
Classification
Regression
Performance Metrics
Classification
Confusion Matrix
Precision,Recall, F1 score
Regression
R2,Adjusted R2
MSE,RMSE,MAE

-----------------------------------------------------------------------------------------------------
when should you use Naive Bayes?
Naive Bayes works best when you have small training data set, relatively small features(dimensions).
it can also be used when the dataset is too large but the categorical variables must be independent of each other.
If you have huge feature list, the model may not give you accuracy, because the likelihood would be distributed and may not follow the Gaussian or other distribution.
Naive Bayes is better suited for categorical input variables than numerical variables.
If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data.
Naive Bayes performs well when we have multiple classes and working with text classification.
It requires less model training time.

condition for Naive Bayes to work is that features should be independent of each other - if you understand the domain, then try to analyze how each features are related to each other, 
are they affecting the each others likelihood. if not Naive Bayes can give you good result.

Naive Bayes is that it’s a good algorithm for working with text classification.
When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, 
this makes for a large number of features. 
The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts.
----------------------------------------------------------------------------------------------------------
what is ensembling?
Ensemble methods are techniques that create multiple models and then combine them to produce improved results.
Ensemble methods usually produces more accurate solutions than a single model would. 
This has been the case in a number of machine learning competitions, where the winning solutions used ensemble methods.
There are two ensemble techniques:
- bagging example random forest
- boosting example xgboost,adaboost,Gradient boostingx.

------------------------------------------------------------------------------------------------------------------
Gradient boosting :
Gradient boosting is a machine learning technique for regression and classification problems, 
which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.
steps in gradient boosting:

1.calculate the avg of all the outputs and give the avg value as predicted value for all the values and 
 calculate loss(actual-predicted) known as residuals.
2.now take  input feature samples  and the calculated residuals as output and give this to train the decision tree which is residual model1.
3.The decision tree(RM1) predicts some values
4.new predicted value=avg predicted value in step1+learning rate*value predicted from decision tree.
5. calculate new residuals= actual values- new predicted values
6. now take  input feature samples  and the  newly calculated residuals as output and give this to train the decision tree which is residual model2.
7.we follow the above steps until the error decreases to minimum value(close to zero) or create n of such residual models.

gradient boosting regression reference:
https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4
https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d
https://towardsdatascience.com/gradient-boosted-decision-trees-explained-9259bd8205af
https://www.youtube.com/watch?v=0ARLObJJ3y4

gradient boosting classification reference:
https://towardsdatascience.com/gradient-boosting-classification-explained-through-python-60cc980eeb3d
---------------------------------------------------------------------------------------------------------------------------
Ordered Encoding in Catboost:
Catboost makes use of the forumula:
cur count+prior/max_count+1, 
current count determines how many previous data points belong to present data point category and from them how many have same target as the present datapoint target has.
max_count= total number of values for that particular category before the present datapoint
prior value will be by default 0.5
"1. in the first step cat boost will shuffle the data.
2. lets consider first sample and has a label of 1 for the first datapoint cur_count is 0 as there are no previous values for that particular category labeled as 1 
and max_count is also zero as there are no previous data points with that particular category.
3. For encoding x2 cat boost will look at whether the present data point category is the same as the previous data point category:
if yes: it will check whether it is labeled as 1 if yes cur_count will be 1 and max_count=1
if no: cur_count will be 0 and max_count will be 0"

---------------------------------------------------------------------------------------------------------------------
Symmetric trees in Catboost:
the concept of symmetric trees in CatBoost refers to a specific way decision nodes are split at each level of a tree. 
Unlike traditional gradient boosting algorithms where each node can have a different split condition, 
CatBoost enforces a symmetric structure within each level of the decision trees it builds. 
This can have a positive impact on training efficiency and generalization.
the symmetric structure means that all decision nodes at the same level share the same split condition.

----------------------------------------------------------------------------------
missing values in catboost:
Missing values are a common problem in real-world datasets. 
Traditional gradient boosting frameworks require imputing missing values before training the model. CatBoost, however, can handle missing values automatically. 

-----------------------------------------------------------------------------------------------------------
xg boost:
xg boost regression:
xg boost is the extension of gradient boosting.
XGBoost was found in order to address the limitations in the gradient boosting, such as slow computation and vulnerability to overfitting. 
This was introduced because gradient boosting was too slow to implement on large datasets sequentially.
it creates decision trees parallely and supports parallelisation and it supports distributed computing to work on large and complex problems
it also uses core computing in order to analyse huge and varied datasets.
XGBoost incorporates several innovative features, including regularization techniques and a customized loss function, making it computationally efficient, scalable, and robust.
it uses cache optimization to make best use of your resources.

ƛ -> regularization parameter
Ɣ -> for auto tree pruning
eta -> how much model will converge
Now calculate the similarity score,

similarity score for regression:
Similarity Score(S.S.) =  (S.R ^ 2) / (N + ƛ)
Here, S.R is the sum of residuals,
N is Number of Residuals.

similarity score for classification:
Similarity Score(S.S.) =  (S.R ^ 2) / Summation(1-probability)+ƛ

so probability is taken for each datapoint.
suppose if there are 10 data points and prob is 0.5(for binary classifcation) then we take(1-0.5) ten times. 
like (1-0.5)+(1-0.5)+......+ƛ

steps:

1. lets say we are dealing with a regresson,in the first step Take the avg of the output feature and let it be A1, subtract A1 from actual output let it be R1(residual).
2. Now first base model is build by considering input featurea and and R1 and target.
3.  R1 values are taken as root node and we split the residuals using one feature, for a categorical feature even if there are n categories we only split into two nodes,
when there are 3 categories in a categorical data we keep 2 categories in one node and other in second node,if the feature is a continous we find the threshold and split.
3. calculate the similarity score of residuals for each split and the root node
As we increase the value of ƛ similarity score decreases so we need to carefully assign a value to ƛ.
4. calculate the Gain where Gain=(sum of similarity scores)-similarity score of residuals of  there parent node.
5. Here the Gamma value will be provided initially if the Gain is more than Gamma splitting will happpen other wise splitting stops there.
   Gamma does the auto pruning. ƛ helps in reducing the overfitting. XGboost calls the learning rate as eta and its value is set as  a hyperparameter.
6. we will try to calculate gain with each feature initially and go with the one with the highest information gain. 
   in this way we will construct a decision tree.After creating a decision tree now what we do is we try to test the decision tree with the same data.
   decision tree will predict some value. we calculate the new predicted value using the below formula.
7. New predicted value=avg predicted value in step1+learning rate*value predicted from decision tree.
8. calculate new residuals= actual values- new predicted values.
9. now take  input feature samples  and the  newly calculated residuals as output and give this to train the decision tree which is residual model2.
10.we follow the above steps until the error decreases to minimum value(close to zero) or create n of such residual models.


Note: we  have a term called cover value which is in the denominator os similarity score formula:PR(1-PR), if the information gain is less than cover value 
for a split, then we wont further split that node. By this method, auto tree pruning will be achieved. The greater the Ɣ value more pruning will be done.

https://www.youtube.com/watch?v=w-_vmVfpssg
we can stop splitting using Gamma, max depth,max leaf nodes, n_estimators.
ƛ value helps the model reduce overfitting and also robust to outliers to some extent.

-----------------------------------------------------------------------------------------------------------------------------------
light GBM:
light GBM is a supervised machine learning algorithm which can be used on Classification and regression problems.
Light GBM is an ensembling technique which comes under gradient boosting family.
Light GBM is fast,robust and consumes less memory and is very powerful when dealing with huge datasets.

Steps in construction of LGBM tree:

1. The process starts with a single leaf node, representing the entire training dataset.
2. The initial prediction for this leaf node is set to the mean (or another appropriate statistic) of the target variable for all the samples in the training dataset.
3. LightGBM uses a histogram-based method for finding the best split points. 
	It discretizes the feature values into discrete bins and constructs histograms to find optimal splits efficiently.
4. Unlike depth-wise growth in some other tree algorithms, LightGBM uses a leaf-wise growth strategy. 
	It grows the tree by expanding the leaf node that provides the maximum reduction in the loss.
5.This strategy can lead to faster convergence and lower memory usage, but it may result in more leaves and shallower trees.
6.LightGBM incorporates regularization techniques to control overfitting. Regularization parameters, such as maximum depth, minimum child samples, and others, 
  are used to limit the complexity of the trees.
7.LightGBM is based on gradient boosting, where each tree is trained to correct the errors made by the previous trees.
The learning process involves updating the leaf values of the tree based on the negative gradient of the loss function with respect to the current predictions.


LightGBM with Example:

We start with a single leaf node that represents the entire dataset.
The initial prediction for this node is set to the mean of the target variable, which is mean(y)=(10+12+14+20+22)/5=15.6
LightGBM examines each feature to find the best split. Let's say it chooses the feature X and the split point 7.5
The data is split into two subsets based on this split point: {3.0, 4.0, 6.0} and {9.0, 11.0}.
LightGBM may decide to grow the tree further. It might choose to split one of the leaves, say the left one, based on another feature or continue growing the right leaf.
Let's say it decides to grow the right leaf. The prediction for the right leaf is already quite accurate (21.0), so further splits may not be necessary.
This process of splitting and growing continues until a stopping criterion is met (e.g., a maximum depth is reached, or further splits don't significantly improve the loss).


how does lgbm decide to further split a leaf node or not?
Gain=((gain or left node/gradient of that node+lambda) +(again of right node/gradient of that node+lambda)-(gain of parent node/gradient of that node+lambda))-lambda
lambda is a regularization parameter.

if the gain is less than a threshold then that leaf node will not be further split. or when a max-depth is reached.

--------------------------------------------------------------------------------------------------------------
CatBoost:
The process starts with a single leaf node that represents the entire training dataset.
The initial prediction for this node is set to the mean (or another appropriate statistic) of the target variable for all the samples in the training dataset.
CatBoost uses a technique called Ordered Boosting to handle categorical features efficiently.
For each feature, it finds a split point that maximizes the reduction in the loss function (e.g., mean squared error for regression, logarithm of the likelihood loss for classification).
For categorical features, it considers an order based on the target variable's mean or other statistics within each category.



Difference between XGboost and Gradient boosting.

XGBoost stands for Extreme Gradient Boosting. XGBoost is a specific implementation of the Gradient Boosting method which delivers 
more accurate approximations by using the strengths of second order derivative of the loss function, 
L1 and L2 regularization and parallel computing. 

XGBoost is particularly popular because it has been the winning algorithm in a number of recent Kaggle competitions.

XGBoost is more regularized form of Gradient Boosting. XGBoost uses advanced regularization (L1 & L2), 
which improves model generalization capabilities.

XGBoost delivers high performance as compared to Gradient Boosting. Its training is very fast and can be parallelized / distributed across clusters.

XGBoost computes second-order gradients, i.e. second partial derivatives of the loss function, 
which provides more information about the direction of gradients and how to get to the minimum of our loss function.

XGBoost also handles missing values in the dataset. So, in data wrangling, you may or may not do a separate treatment for the missing values,
because XGBoost is capable of handling missing values internally.
 
---------------------------------------------------------------------------------------------------------------
Difference between Light GBM and xgboost:

There has been only a slight increase in accuracy and auc score by applying Light GBM over XGBOOST but there is a significant 
difference in the execution time for the training procedure. 
Light GBM is almost 7 times faster than XGBOOST and is a much better approach when dealing with large datasets.

------------------------------------------------------------------------------------------------------------------
while working which algorithm you would choose?

1. Size of the training data

 if the training data is smaller or if the dataset has a fewer number of observations and a higher number of features like genetics or textual data, 
 choose algorithms with high bias/low variance like Linear regression, Naïve Bayes, or Linear SVM.

If the training data is sufficiently large and the number of observations is higher as compared to the number of features, 
one can go for low bias/high variance algorithms like KNN, Decision trees, or kernel SVM.

2. Accuracy and/or Interpretability of the output

A highly interpretable algorithm (restrictive models like Linear Regression) means that one can easily 
understand how any individual predictor is associated with the response while the flexible models 
give higher accuracy at the cost of low interpretability.

linear,logisitic,decision tree,naive bayes.

high accurate
random forest,boosting, neural networks.


3. Speed or Training time
Higher accuracy typically means higher training time. Also, algorithms require more time to train on large training data. 
In real-world applications, the choice of algorithm is driven by these two factors predominantly.

Algorithms like Naïve Bayes and Linear and Logistic regression are easy to implement and quick to run. 
Algorithms like SVM, which involve tuning of parameters, Neural networks with high convergence time, and random forests, 
need a lot of time to train the data.


4. Linearity
Many algorithms work on the assumption that classes can be separated by a straight line (or its higher-dimensional analog). 
Examples include logistic regression and support vector machines. Linear regression algorithms assume that data trends follow a straight line. 
If the data is linear, then these algorithms perform quite good.

However, not always is the data is linear, so we require other algorithms which can handle high dimensional and complex data structures. 
Examples include kernel SVM, random forest, neural nets.

The best way to find out the linearity is to either fit a linear line or run a logistic regression or SVM and check for residual errors. 
A higher error means the data is not linear and would need complex algorithms to fit.


https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html

---------------------------------------------------------------------------------------------------------------------------
Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa.
Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. 
For multiclass problem you will need to reduce it into multiple binary classification problems.

Random Forest works well with a mixture of numerical and categorical features. 
When features are on the various scales, it is also fine. Roughly speaking, with Random Forest you can use data as they are. 
SVM maximizes the "margin" and thus relies on the concept of "distance" between different points. 
It is up to you to decide if "distance" is meaningful.


SVM works well on the textual data.
Decision trees have better interpretability, they work faster and if you have categorical/numerical variables its fine,
moreover: non-linear dependencies are handled well (given N large enough). 
Also they train faster than SVM in general, but they have tendency to overfit
----------------------------------------------------------------------------------------------------------------------

Time series Analysis:
Time series analysis is that type of analysis that deals with time series data.

stationary process:
A stationary process is said to be stationary only if the mean and variance is constant over the time, 
and if the value of the covariance between the two time periods mainly depends upon the distance between the two time periods.

Stationary time series is when the mean and variance are constant over time. 
Stationarity means that the statistical properties of a time series (or rather the process generating it) do not change over time. 
It is easier to predict when the series is stationary.
‘Stationarity’ is one of the most important concepts you will come across when working with time series data. 
A stationary series is one in which the properties – mean, variance and covariance, do not vary with time.

Why does Time Series(TS) need to be stationary? 
A. It is because of the following reasons: 
a) If a TS has a particular behavior over a time interval, then there's a high probability that over a different interval, it will have same behavior,provided TS is stationary. 
   This helps in forecasting accurately. 
b) Theories & Mathematical formulas are more mature & easier to apply for as TS which is stationary.
 
------------------------------------------------------------------------------------------------------------------

component in time series:

Level: The average value in the series.
Trend: The increasing or decreasing value in the series.
Seasonality: The repeating short-term cycle in the series.
Noise: The random variation in the series.

----------------------------------------------------------------------------------------------------
Steps in Time Series:

Step 1: Visualize the Time Series. It is essential to analyze the trends prior to building any kind of time series model.
Step 2: Stationarize the Series. 
Step 3: Find Optimal Parameters. 
Step 4: Build ARIMA Model. 
Step 5: Make Predictions.

---------------------------------------------------------------------------------------------------------
ACF(auto correleation function)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Autocorrelation is a measure of the correlation between the lagged values of a time series.
For example, r1 is the autocorrelation between yt and yt-1; similarly, r2 is the autocorrelation between yt and yt-2. 
					or 
The autocorrelation function (ACF) defines how data points in a time series are related, on average, to the preceding data points.
In other words, it measures the self-similarity of the signal over different delay times.

A time series can have components like trend, seasonality, cyclic and residual. 
ACF considers all these components while finding correlations hence it’s a ‘complete auto-correlation plot’.
 
 
PACF(partial autocorrelation function)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

pacf is useful in detection the order of AR process.

- PACF is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, 
it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) 
with the next lag value hence ‘partial’ and not ‘complete’ as we remove already found variations before we find the next correlation. 
So if there is any hidden information in the residual which can be modeled by the next lag, 
we might get a good correlation and we will keep that next lag as a feature while modeling. 
Remember while modeling we don’t want to keep too many features which are correlated as that can create multicollinearity issues. 
Hence we need to retain only the relevant features.


Example:

 Consider three variables: X,Y,Z.

Let's say Z has a direct influence on the variable X. You can think of Z as some economic parameter in US which is influencing some other economic parameter X of China.

Now it may be that a parameter Y (some parameter in England) is also directly influenced by Z. 
But there is an independent relationship between X and Y as well. By independence here I mean that this relationship is independent from Z.

So you see when Z changes, X changes because of the direct relationship between X and Z, and also because Z changes Y which in turn changes X. So X changes because of two reasons.

Now read this with Z=yt−h,  Y=yt−h+τ and X=yt (where h>τ).

Autocorrelation between X and Z will take into account all changes in X whether coming from Z directly or through Y.

Partial autocorrelation removes the indirect impact of Z on X coming through Y.

----------------------------------------------------------------------------------------------------
what is an auto regression model?
Autoregression is a time series model that uses observations from previous time steps as input to a regression equation 
to predict the value at the next time step. 
It is a very simple idea that can result in accurate forecasts on a range of time series problems.
------------------------------------------------------------------------------------------------
Moving average:
This model simply states that the next - observation is the mean of all past observations.
- The moving average model is probably the most naive approach to time series modelling. 


- Although simple, this model might be surprisingly good and it represents a good starting point.
- Otherwise, the moving average can be used to identify interesting trends in the data. 
We can define a window to apply the moving average model to smooth the time series, and highlight different trends.
- we take a window size of k at a time and perform some desired mathematical operation on it. 
A window of size k means k consecutive values at a time. In a very simple case all the ‘k’ values are equally weighted.
---------------------------------------------------------------------------------------------------------------------
## ARIMA(AUTO REGRESSIVE(p) INTEGRATED(d) MOVING AVERAGE(q))
- ARIMA models are denoted with the notation ARIMA(p, d, q). 
- These three parameters account for seasonality, trend, and noise in data.


ARIMA, short for ‘Auto Regressive Integrated Moving Average’ is actually a class of models that ‘explains’ a given time series 
based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.

Any ‘non-seasonal’ time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models.

An ARIMA model is characterized by 3 terms: p, d, q

where,

- p is the order of the AR term

- q is the order of the MA term

- d is the number of differencing required to make the time series stationary

### 3. What does the p, d and q in ARIMA model mean?
The first step to build an ARIMA model is to make the time series stationary.

Why?

Because, term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors. 
Linear regression models, as you know, work best when the predictors are not correlated and are independent of each other.

So how to make a series stationary?

The most common approach is to difference it. 
That is, subtract the previous value from the current value. 
Sometimes, depending on the complexity of the series, more than one differencing may be needed.

The value of d, therefore, is the minimum number of differencing needed to make the series stationary. 
And if the time series is already stationary, then d = 0.

Next, what are the ‘p’ and ‘q’ terms?

‘p’ is the order of the ‘Auto Regressive’ (AR) term. It refers to the number of lags of Y to be used as predictors. 
And ‘q’ is the order of the ‘Moving Average’ (MA) term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.

###  Steps
- Step 1 — Check stationarity: If a time series has a trend or seasonality component, it must be made stationary before we can use ARIMA to forecast. .
- Step 2 — Difference: If the time series is not stationary, it needs to be stationarized through differencing. 
Take the first difference, then check for stationarity. Take as many differences as it takes. Make sure you check seasonal differencing as well.
- Step 3 — Filter out a validation sample: This will be used to validate how accurate our model is. Use train test validation split to achieve this
- Step 4 — Select AR and MA terms: Use the ACF and PACF to decide whether to include an AR term(s), MA term(s), or both.
- Step 5 — Build the model: Build the model and set the number of periods to forecast to N (depends on your needs).
- Step 6 — Validate model: Compare the predicted values to the actuals in the validation sample.
-------------------------------------------------------------------------------------------------

Rolling mean:
Rolling mean can show us whether the data is stationary or not. 
which will let us know whether the data has constant mean and std at different time intervals
Rolling calculations simply apply functions to a fixed width subset of this data (aka a window), 
indexing one observation each calculation. There are a few common reasons you may want to use a rolling calculation in time series analysis:

Measuring the central tendency over time (mean, median)
Measuring the volatility over time (sd, var)
Detecting changes in trend (fast vs slow moving averages)
Measuring a relationship between two time series over time (cor, cov)

rolling(4) means we are taking the mean of the last four entries at that particular point of time. same with std.


-------------------------------------------------------------------------------------------------------------------
statistical test:

ADF (Augmented Dickey Fuller) Test The Dickey Fuller test is one of the most popular statistical tests. 
It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. 
The null and alternate hypothesis of this test are:

Null Hypothesis: The series has a unit root (value of a =1)

Alternate Hypothesis: The series has no unit root.

If we fail to reject the null hypothesis, we can say that the series is non-stationary. 
This means that the series can be linear or difference stationary.

If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). 
When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary).



-----------------------------------------------------------------------------------------------------------
Differencing:

The second (and most important) step in fitting an ARIMA model is the determination of the order of differencing needed to stationarize the series. 
Normally, the correct amount of differencing is the lowest order of differencing that yields a time series which fluctuates 
around a well-defined mean
value and whose autocorrelation function (ACF) plot decays fairly rapidly to zero, either from above or below. 
If the series still exhibits a long-term trend, or otherwise lacks a tendency to return to its mean value, 
or if its autocorrelations are are positive out to a high number of lags (e.g., 10 or more), 
then it needs a higher order of differencing. We will designate this as our "first rule of identifying ARIMA models" :

Rule 1: If the series has positive autocorrelations out to a high number of lags, then it probably needs a higher order of differencing.
Rule 2: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, 
then the series does not need a higher order of differencing. 
If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced. BEWARE OF OVERDIFFERENCING!!

Rule 3: The optimal order of differencing is often the order of differencing at which the standard deviation is lowest.

-------------------------------------------------------------------------------------------------------------------------------------------------

Transformation:


Transformations are used to stabilize the non-constant variance of a series. 
Common transformation methods include power transform, square root, and log transform. 

Arima assumptions:

Normally in an ARIMA model, we make use of either the AR term or the MA term. We use both of these terms only on rare occasions. 
We use the ACF plot to decide which one of these terms we would use for our time series
- If there is a Positive autocorrelation at lag 1 then we use the AR model
- If there is a Negative autocorrelation at lag 1 then we use the MA model


Use AR terms in the model when the

ACF plots show autocorrelation decaying towards zero
PACF plot cuts off quickly towards zero
ACF of a stationary series shows positive at lag-1

Use MA terms in the model when the model is

Negatively Autocorrelated at Lag — 1
ACF that drops sharply after a few lags
PACF decreases more gradually.


What is seasonality in Time series and how can you deal with different types of seasonality in time series modeling?   

 

When time series shows a repeated pattern over time, Seasonality in time series occurs. 
For example, sales of ice-cream decrease during winter season, sales of coolers increase during the summers, etc.

Seasonality makes time series non-stationary because average value of the variables at different time periods.

The best method of removing seasonality from a time series is Differentiating a time series 
(calculating the numerical difference between a particular value and a value with a periodic lag).

-----------------------------------------------------------------------------------------------------------------------------------------
The various process you can use to find out your data is stationary or not by the following terms: 
1. Visual Test 
2. Statistical Test 
3. ADF(Augmented Dickey-Fuller) Test 
4. KPSS(Kwiatkowski-Phillips-Schmidt-Shin) Test
-------------------------------------------------------------------------------------------------------------------------------------
Exponential smoothing is a way to smooth out data for presentations or to make forecasts. 
It's usually used for finance and economics. If you have a time series with a clear pattern, you could use moving averages — 
but if you don't have a clear pattern you can use exponential smoothing to forecast.

Forecasts produced using exponential smoothing methods are weighted averages of past observations, 
with the weights decaying exponentially as the observations get older. 
In other words, the more recent the observation the higher the associated weight.

------------------------------------------------------------------------------------
Level: The average value in the series.
Trend: The increasing or decreasing value in the series.
Seasonality: The repeating short-term cycle in the series.
Noise: The random variation in the series.
-------------------------------------------------------------------------------------------------------------------------
what is customer segmentation?

Customer Segmentation is the process of dividing customers into groups based on common characteristics so companies can market to each group effectively and appropriately.

What is customer segmentation in ML?
Customer segmentation is the method of distributing a customer base into collections of people based on mutual characteristics so organizations 
can market to group efficiently and competently individually.
----------------------------------------------------------------------------------------------------------------------------------
explain clustering? Types of clustering? how do you evaluate clustering?
Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.

Clustering is a Machine Learning technique that involves the grouping of data points. 
Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. 
In theory, data points that are in the same group should have similar properties and/or features, 
while data points in different groups should have highly dissimilar properties and/or features.

-------------------------------------------------------------------------------------------------------------------
Types of Clustering:

Broadly speaking, clustering can be divided into two subgroups :
• Hard Clustering: 
	In hard clustering, each data point either belongs to a cluster completely or not. 
	In hard clustering, data points are grouped into distinct clusters in a way that there is no overlap between clusters. 
	This means that a data point belongs to one and only one cluster, and there is no ambiguity in the assignment.
	Common examples of hard clustering algorithms include k-means clustering and hierarchical clustering.
• Soft Clustering: 
	In soft clustering, instead of putting each data point into a separate cluster, 
	a probability or likelihood of that data point to be in those clusters is assigned. 
	Soft clustering, also known as fuzzy clustering, is a clustering technique that allows data points to belong to multiple clusters to varying degrees. 
	Unlike hard clustering, where there is a clear and exclusive assignment, soft clustering provides a measure of membership or affiliation for each data point in every cluster.
	In soft clustering, each data point is associated with a vector of cluster membership coefficients. 
	These coefficients indicate the extent to which the data point belongs to each cluster. The sum of coefficients for each data point is often normalized to 1.

A few Types of Clustering Algorithms
•	Connectivity Models
As the name indicates, connectivity models tend to classify data points based on the closeness of data points. 
It is based on the notion that the data points that are closer to each other depict more similar characteristics as compared to those placed farther away. 
The algorithm supports an extensive hierarchy of clusters that might merge with each other at certain points. 
It is not limited to a single partitioning of the dataset. 
The choice of distance function is subjective and may vary with each clustering application. 
There are also two different approaches to address a clustering problem with connectivity models. 
First is where all data points are classified into separate clusters and then aggregated as the distance decreases. 
The second approach is where the whole dataset is classified as one cluster and then partitioned into multiple clusters as the distance increases. 
Even though the model is easily interpretable, it lacks scalability to process bigger datasets. 


•	Distribution Models
With a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.
It works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.
If you aren't sure of how the distribution in your data might be, you should consider a different type of algorithm. 
A well-known example of this model is the expectation-maximisation algorithm.

This is a type of clustering method where data points are grouped into clusters based on the assumption that they come from different statistical distributions or models. 
One common example of distribution-based clustering is the Gaussian Mixture Model (GMM) clustering algorithm.

Assumption of Different Distributions: The main idea is that each cluster follows a certain probability distribution (like a Gaussian distribution) with its own mean and variance.

Probability of Belonging: Instead of assigning points directly to clusters, distribution-based clustering assigns probabilities of points belonging to different clusters.

•	Density Models
These models search the data space for varied density of data points and isolates the different density regions. 
It then assigns the data points within the same region as clusters. 
DBSCAN and OPTICS are the two most common examples of density models. 

•	Centroid Models
Centroid models are iterative clustering algorithms where similarity between data points are derived based on their closeness to the centroid of the cluster.
The centroid (centre of the cluster) is formed making sure that the distance of the data points is minimum with the centre. 
The solution for such clustering problems are usually approximated over multiple trials. 
An example of centroid models is the K-means algorithm.


 
------------------------------------------------------------------------------------------------------------------------
K-Means Clustering
K-Means is probably the most well-known clustering algorithm. 
K-means clustering is an unsupervised ML algorithm that organizes the data into various clusters.
Each datapoint belongs to one single cluster there will not be any overlapping subgroups.
It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. 
It assigns data points to a cluster such that the sum of the squared distance between the data points and the 
cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. 
The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.

The approach kmeans follows to solve the problem is called Expectation-Maximization. 
The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster.

STEPS IN KMEANS CLUSTERING:

1.Specify number of clusters K.
2.Initialize centroids, these centroids serve as initial point of centroids. The choice of centroids will impact the result.
3.For each data point, calculate the distance to each centroid. Assign the data points to any one cluster whose centroid is closest (using a distance metric like Euclidean distance).
4.Recalculate the centroids of the clusters based on the data points assigned to them. The new centroid is the mean (average) of all the data points in the cluster.
5. After updating the value of the centroids, we again use the same dataset and assign each datapoint to a cluster.
6.This process continues until a maximum number of iterations is reached.

----------------------------------------------------------------------------------------------------------------------------------------------------

Assumptions of K-means clustering:
K-means assumes that the clusters have similar sizes and shapes.
 The algorithm assumes that the clusters are roughly of equal sizes. 
 This is because K-means assigns an equal number of data points to each cluster, which can lead to imbalanced clusters if the sizes are very different.
 K-means works best when the clusters are well-separated and have clear boundaries. It may struggle when clusters are overlapping or have irregular shapes.
 K-means assumes that the clusters have similar densities, which means that the data points within each cluster are roughly evenly distributed.
 K-means is designed for numerical data. It computes distances between data points using features' values. It's not directly suitable for categorical or mixed data types.
 Outliers can significantly affect the placement of centroids and the assignment of data points. K-means is sensitive to outliers and might not handle them well.
 K-means requires you to specify the number of clusters (K) in advance. If you don't have a good estimate of K, it can be challenging to determine the right number.
 
------------------------------------------------------------------------------------------------------------------------------------
 
How is the initial placement of centroids determined in K-means?
Random Initialization: 
~~~~~~~~~~~~~~~~~~~~~~
The simplest approach is to randomly select K data points from the dataset as initial centroids. 
While this method is easy to implement, it can lead to different results on different runs and might not always yield optimal clusters.

Manual Initialization:
~~~~~~~~~~~~~~~~~~~ 
In some cases, domain knowledge might suggest initial centroids. 
For example, if you have some idea about where clusters might be located, you can use that information to set initial centroids.

K-Means++ Initialization:
~~~~~~~~~~~~~~~~~~~~~~~
Select the First Centroid: Choose the first centroid randomly from the dataset. This is the starting point of the initialization.

Compute Distances: Calculate the squared distances from each data point to the nearest existing centroid. 
The distance for each point is the minimum squared Euclidean distance to any existing centroid.

Choose Next Centroid: 
Now, you want to choose the next centroid. The probability of a data point being selected is higher if it's far away from the existing centroids. The farther, the better.
The probability is determined by the square of the distance. So, points that are farther away will have a higher probability of being chosen.

Repeat: Repeat steps 2 and 3 until you have selected K initial centroids.

Custom Initialization:
~~~~~~~~~~~~~~~~~~~~~~
You can also use domain-specific knowledge to initialize centroids. This may involve using prior information about the data distribution or selecting centroids based on certain characteristics.
---------------------------------------------------------------------------------------------------------------------------------
what is the purpose of lloyds algorithm in k-means?
The Lloyd's algorithm, also known as the K-means algorithm, updates the centroids of clusters by calculating the mean of the points assigned to each cluster.

----------------------------------------------------------------------------------------------------------------

How to find the optimal number of clusters?
Elbow Method: 
For each K value, run the K-means algorithm on the dataset and compute the WCSS.
For each K value, calculate the sum of squared distances from each data point to its assigned cluster centroid. This gives you the WCSS for that specific K.
Plot the within-cluster sum of squares (WCSS) as a function of K. WCSS measures the total variance within each cluster. 
The idea is that as you increase K, WCSS will decrease because more clusters mean each point is closer to its centroid. 
However, after a certain point, adding more clusters won't significantly reduce WCSS, leading to an "elbow" in the plot. 
The optimal K is where the rate of decrease sharply changes (the elbow point).

Silhouette Score: 
Silhouette score is one of the technique which we use to validate the clusters.

For each data point i, calculate the average distance from all other data points in the same cluster. Denote this distance as ai, representing cohesion.
Calculate the average distance from the data point i to all data points in the nearest neighboring cluster. Denote this distance as bi, representing separation.
silhouette score: bi-ai/max(bi,ai)
The silhouette score ranges from -1 to 1:


​
 si=1 indicates that the data point is well matched to its own cluster and poorly matched to neighboring clusters.

​
 si=0 indicates that the data point is on or very close to the decision boundary between two neighboring clusters.

​
 si=−1 indicates that the data point might be better suited to a different cluster.

Cross-Validation: If you're using clustering for some downstream task (e.g., classification), 
you can perform cross-validation on different numbers of clusters and see which K leads to the best performance on your task.

Domain Knowledge: 
Sometimes, domain knowledge can guide you to a reasonable number of clusters based on your understanding of the data and the problem you're solving.

----------------------------------------------------------------------------------------------------------------------------
How does K-means deal with outliers and noise?
K-means clustering can be sensitive to outliers and noise. Outliers can pull centroids towards them, distorting clusters. 
Misallocated outliers and noise can affect cluster quality. Preprocessing methods like outlier removal or data transformation can mitigate their impact. 
More robust K-means variations or considering other methods like DBSCAN might better handle datasets with outliers and noise.

------------------------------------------------------------------------------------------------------------------
Can K-means be used for clustering in high-dimensional spaces? If yes, what challenges might arise?
Yes, K-means can be used for clustering in high-dimensional spaces, but there are challenges associated with doing so:

Curse of Dimensionality: 
As the number of dimensions increases, the data points become more spread out, and the concept of "distance" loses its meaning. 
This can lead to clusters appearing more uniformly distributed and affecting the effectiveness of K-means.

Increased Computational Complexity: 
High-dimensional spaces require more computations to calculate distances, which can slow down the algorithm significantly.


Sparse Data: In high dimensions, data points tend to become sparse, meaning most dimensions have few non-zero values. 
This can make it difficult to measure meaningful distances and find clusters.

Curse of Interpretability: 
As the number of dimensions increases, it becomes harder to visualize and interpret clusters.

Overfitting: 
In high-dimensional spaces, K-means might be prone to overfitting, where each data point forms its own cluster due to increased distance variations.

----------------------------------------------------------------------------------------------------------
what if we get 20 clusters, what does it say?

Fine-Grained Patterns: The data is very complex and exhibits subtle variations, which K-means has captured by creating numerous clusters.

Overfitting: Having too many clusters might lead to overfitting, where K-means assigns individual points to their own clusters, losing the general patterns present in the data.

Incorrect K Selection: The chosen value of K might be too high. 
It's important to use methods like the elbow method, silhouette score, or cross-validation to determine the optimal number of clusters.

High-Dimensional Data: In high-dimensional spaces, clusters might appear more fragmented due to the "curse of dimensionality."

--------------------------------------------------------------------------------------------------------
Steps in Elbow method:

Run K-means: Apply the K-means algorithm to your data for a range of K values (e.g., from 1 to 10 clusters).

Calculate WCSS: For each value of K, calculate the sum of squared distances between data points and their assigned cluster centroids. This value is the WCSS.

Plot the Elbow Curve: Create a plot with K values on the x-axis and the corresponding WCSS values on the y-axis.

Identify the Elbow Point: Examine the plot. The idea is to look for the point where the WCSS starts to decrease more slowly, forming an "elbow." This point indicates that adding more clusters doesn't lead to a significant reduction in WCSS.

Choose K: The K value corresponding to the elbow point is often considered as the optimal number of clusters. 
However, keep in mind that the choice of K can be somewhat subjective, and you might need to balance it with domain knowledge and other evaluation methods.


Interpretation:

If K is too small, the WCSS will be high because data points are far from centroids.
As K increases, WCSS generally decreases because clusters become tighter.
The elbow point represents a trade-off between lower WCSS (better fit to data) and a more reasonable number of clusters.


------------------------------------------------------------------------------------------------------


Calculate the Average Distance to Other Points in the Same Cluster (a_i):
For each data point, calculate the average distance to all other data points within the same cluster. This represents how similar the point is to other points in its cluster.

Calculate the Average Distance to the Nearest Cluster (b_i):
For each data point, calculate the average distance to all data points in the nearest cluster that the point is not a part of. 
This represents how dissimilar the point is to points in the nearest neighboring cluster.

Calculate the Silhouette Score for Each Point (s_i):
For each data point, calculate the silhouette score using the formula:

bi-ai/max(bi,ai)

The silhouette score ranges from -1 to 1. A high positive score indicates that the point is well inside its own cluster and far from neighboring clusters. 
A score near 0 suggests that the point is on or very close to the boundary between clusters. 
A negative score indicates that the point might have been assigned to the wrong cluster.
 
​Calculate the Overall Silhouette Score:
Calculate the average silhouette score across all data points to get the overall silhouette score for the entire clustering solution.

Interpret the Silhouette Score:

A higher silhouette score indicates better-defined, well-separated clusters.
A score near 0 suggests overlapping or ambiguous clusters.
Negative scores indicate that data points might have been assigned to the wrong clusters.

----------------------------------------------------------------------------------------
can we use k-means on categorical data?
K-means is not designed for categorical data. To use K-means with categorical data, 
you need to transform it into a numerical format (e.g., one-hot encoding) and then apply K-means.





-------------------------------------------------------------------------------------------------------
Intra-Cluster Distance:
Intra-cluster distance refers to the average or total distance between data points within the same cluster. It measures how tightly grouped the data points are within a cluster. 
The lower the intra-cluster distance, the more similar the data points are to each other within the cluster. 

Inter-Cluster Distance:
Inter-cluster distance refers to the distance between centroids of different clusters. 
It measures how distinct or separated the clusters are from each other. A higher inter-cluster distance indicates that the centroids are far apart, and the clusters are well-separated. In K-means, maximizing the inter-cluster distance helps ensure that the clusters are distinct and do not overlap.

Implications:

K-means aims to minimize the intra-cluster distance while maximizing the inter-cluster distance.
When intra-cluster distance is small and inter-cluster distance is large, it suggests that the clusters are well-defined and separated.
The ideal scenario is tight clusters with low intra-cluster distances and distinct clusters with high inter-cluster distances.
K-means Optimization:
The K-means algorithm iteratively adjusts centroids to minimize the sum of squared distances between data points and their respective centroids, 
effectively reducing the intra-cluster distance. It does this while simultaneously trying to maximize the inter-cluster distance by positioning centroids far apart.

Overall, understanding these concepts helps in evaluating the quality of clusters generated by K-means and in assessing how well the algorithm has grouped 
similar data points and separated different groups in the data.

-------------------------------------------------------------------------------------------------------
Can K-means handle non-spherical clusters effectively? Why or why not?
K-means assumes that clusters are spherical and have equal variances. 
It calculates distances based on the Euclidean distance metric, which is sensitive to the scale and orientation of data dimensions. 
As a result, K-means tends to perform well when clusters are relatively round and have similar sizes.
---------------------------------------------------------------------------------------------------------
objective of K-means:
The objective of K-means clustering is to minimize the sum of squared distances between data points and their assigned cluster centroids. 
This is known as the "within-cluster sum of squares" or "inertia."

-------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------

K-Means has the advantage that it’s pretty fast, as all we’re really doing is computing the distances between points and group centers; 
very few computations! It thus has a linear complexity O(n).

On the other hand, K-Means has a couple of disadvantages:

Firstly, you have to select how many groups/classes there are. 
This isn’t always trivial and ideally with a clustering algorithm we’d want it to figure those out 
for us because the point of it is to gain some insight
from the data. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs
of the algorithm. Thus, the results may not be repeatable and lack consistency. Other cluster methods are more consistent.

when we shouldn't use K-means clustering:

It can not handle noisy data and outliers. 
It is not suitable to identify clusters with non-convex shapes.


when k-means will fail?

-means assume the variance of the distribution of each attribute (variable) is spherical;

all variables have the same variance;

the prior probability for all k clusters are the same, i.e. each cluster has roughly equal number of observations;

 If any one of these 3 assumptions is violated, then k-means will fail.

Applications:

k-means can be applied to data that has a smaller number of dimensions, is numeric, and is continuous.

such as document clustering, identifying crime-prone areas, customer segmentation, insurance fraud detection, public transport data analysis, clustering of IT alerts…etc.




K-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median
vector of the group. This method is less sensitive to outliers (because of using the Median) but is much slower for larger datasets as sorting is required on each iteration when computing the Median vector.
--------------------------------------------------------------------------------------------------------------------------
kmeans algorithm is very popular and used in a variety of applications such as market segmentation, 
document clustering, image segmentation and image compression, etc. 
The goal usually when we undergo a cluster analysis is either: Get a meaningful intuition of the structure of the data we're dealing with.

when not to use k means clustering?
Non-Globular Cluster Shapes: K-Means assumes that clusters are spherical and have equal variance. 
If your data has clusters with irregular shapes, different sizes, or non-uniform variances, K-Means might produce suboptimal results. 
In such cases, algorithms like DBSCAN or hierarchical clustering, which can handle various cluster shapes, might be more appropriate.



--------------------------------------------------------------------------------------------------------------------------
DBSCAN algorithm:
DBSCAN stands for density-based spatial clustering of applications with noise. 
It is able to find arbitrary shaped clusters and clusters with noise (i.e. outliers).

DBSCAN is a clustering method that is used in machine learning to separate clusters of high density from clusters of low density. 
Given that DBSCAN is a density based clustering algorithm, it does a great job of seeking areas in the data that have a high density of observations, 
versus areas of the data that are not very dense with observations. 
DBSCAN can sort data into clusters of varying shapes as well, another strong advantage.
The main idea behind DBSCAN is that a point belongs to a cluster if it is close to many points from that cluster.

There are two key parameters of DBSCAN:
eps: The distance that specifies the neighborhoods. 
Two points are considered to be neighbors if the distance between them is less than or equal to eps.
minPts: Minimum number of data points to define a cluster.

Based on these two parameters, points are classified as core point, border point, or outlier:
Core point: A point is a core point if there are at least minPts number of points (including the point itself) in its surrounding area with radius eps.
Border point: A point is a border point if it is reachable from a core point and there are less than minPts number of points within its surrounding area.
Outlier: A point is an outlier if it is not a core point and not reachable from any core points.

 

steps in DBSCAN:

1.DBSCAN begins with an arbitrary starting data point that has not been visited. 
The neighborhood of this point is extracted using a distance epsilon ε (All points which are within the ε distance are neighborhood points).
2.If there are a sufficient number of points (according to minPoints) within this neighborhood then the clustering process startS
and the current data point becomes the first point in the new cluster. Otherwise, the point will be labeled as noise (later this noisy point might become the part of the cluster). 
In both cases that point is marked as “visited”.
3.For this first point in the new cluster, the points within its ε distance neighborhood also become part of the same cluster. 
This procedure of making all points in the ε neighborhood belong to the same cluster is then repeated for all of the new points that have been just added to the cluster group.
4.This process of steps 2 and 3 is repeated until all points in the cluster are determined i.e all points within the ε neighborhood of the cluster have been visited and labeled.
5.Once we’re done with the current cluster, a new unvisited point is retrieved and processed, leading to the discovery of a further cluster or noise. 
6.This process repeats until all points are marked as visited. 
Since at the end of this all points have been visited, each point will have been marked as either belonging to a cluster or being noise.

DBSCAN poses some great advantages over other clustering algorithms. 
Firstly, it does not require a pe-set number of clusters at all. It also identifies outliers as noises, 
unlike mean-shift which simply throws them into a cluster even if the data point is very different. 
Additionally, it can find arbitrarily sized and arbitrarily shaped clusters quite well.

The main drawback of DBSCAN is that it doesn’t perform as well as others when the clusters are of varying density. 
This is because the setting of the distance threshold ε and minPoints for identifying the neighborhood points will vary from cluster to cluster 
when the density varies. This drawback also occurs with very high-dimensional data since again the distance threshold ε becomes challenging to 
estimate.

Metrics for Measuring DBSCAN’s Performance:
Silhouette Score: The silhouette score is calculated utilizing the mean intra- cluster distance between points, AND the mean nearest-cluster distance. 
For instance, a cluster with a lot of data points very close to each other (high density) AND is far away from the next nearest cluster 
(suggesting the cluster is very unique in comparison to the next closest), will have a strong silhouette score. A silhouette score ranges from -1 to 1, 
with -1 being the worst score possible and 1 being the best score. Silhouette scores of 0 suggest overlapping clusters.

when does DBSCAn doesn't perform well?
High-Dimensional Data: 
DBSCAN's performance can deteriorate in high-dimensional spaces due to the "curse of dimensionality." As the number of dimensions increases, 
the density of data points decreases, making it difficult for DBSCAN to find meaningful clusters. 
In high-dimensional data, algorithms like t-SNE for visualization or spectral clustering might be more suitable.

Global Clusters: 
DBSCAN is particularly effective for detecting local clusters and noise. If you're looking for large, globally distributed clusters that might have significant empty spaces 
between them, other methods like K-Means, hierarchical clustering, or Gaussian Mixture Models could be more appropriate.

Choosing Parameters: 
DBSCAN requires the setting of two key parameters: epsilon (ε) for defining the neighborhood radius and the minimum number of points required to form a cluster (minPts). 
Choosing appropriate values for these parameters can impact the quality of results. If you have difficulty determining these parameters, 
other algorithms with fewer hyperparameters might be easier to work with.

---------------------------------------------------------------------------------------------------------------------------------------------------------------






Hirarcheal clustering:
Hierarchical Clustering is categorised into divisive and agglomerative clustering. 
Basically, these algorithms have clusters sorted in an order based on the hierarchy in data similarity observations.

Divisive Clustering or the top-down approach groups all the data points in a single cluster. 
Then it divides it into two clusters with least similarity among each other. 
The process is repeated and clusters are divided until there is no more scope of doing so. 

Agglomerative Clustering or the bottom-up approach assigns each data point as a cluster and aggregate the most similar clusters. 
This essentially means bringing similar data together into a cluster. 
Out of the two approaches, Divisive Clustering is more accurate. But then, 
it again depends on the type of problem and the nature of available dataset to decide which approach to apply
 to a specific clustering problem in Machine Learning. 


Hierarchical clustering algorithms fall into 2 categories: top-down or bottom-up. 
Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters 
until all clusters have been merged into a single cluster that contains all data points. 
Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. 
This hierarchy of clusters is represented as a tree (or dendrogram). 
The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.
in Top down approach initially all the datapoints are considered as one cluster and are divided using a distance metrix


Agglomerative clustering is a hierarchical clustering technique that starts with each data point as its own cluster and then successively merges clusters based on their similarity. It builds a tree-like structure called a dendrogram, which shows the sequence of cluster merges. 
The process continues until all data points are part of a single cluster or until a specified number of clusters is reached.

hierarchical agglomerative clustering:

1.We begin by treating each data point as a single cluster i.e if there are X data points in our dataset then we have X clusters. 
We then select a distance metric that measures the distance between two clusters. 
As an example, we will use average linkage which defines the  distance between two clusters to be the average distance between data points 
in the first cluster and data points in the second cluster.
2.On each iteration, we combine two clusters into one. The two clusters to be combined are selected as those with the smallest average linkage. 
I.e according to our selected distance metric, these two clusters have the smallest distance between each other and therefore are the most similar 
and should be combined.
3.Step 2 is repeated until we reach the root of the tree i.e we only have one cluster which contains all data points. 
In this way we can select how many clusters we want in the end, simply by choosing when to stop combining the clusters 
i.e when we stop building the tree!


Hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best 
since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well 
whereas with other clustering algorithms, the choice of distance metric is critical. 
A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure 
and you want to recover the hierarchy; other clustering algorithms can’t do this. 
These advantages of hierarchical clustering come at the cost of lower efficiency, as it has a time complexity of O(n³), 
unlike the linear complexity of K-Means and GMM.


https://www.youtube.com/watch?v=0jPGHniVVNc

-------------------------------------------------------------------------------------------------------------------------



Single Linkage (Nearest Neighbor):

Distance between two clusters is defined by the shortest distance between any two data points from different clusters.
Tends to create elongated clusters.

Complete Linkage (Farthest Neighbor):

Distance between two clusters is defined by the maximum distance between any two data points from different clusters.
Can create compact, spherical clusters.

Average Linkage:

Distance between two clusters is defined by the average distance between all pairs of data points from different clusters.
Balances the effects of single and complete linkage.

o t
Ward's Linkage:

Measures the increase in the sum of squared distances when two clusters are merged.
Tends to form clusters of relatively equal sizes and minimizes variance within clusters.












-----------------------------------------------------------------------------------------------------------------
cluster evaluation:

two metrics that may give us some intuition about k:
Elbow method
Silhouette analysis
https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc


Elbow Method
Elbow method gives us an idea on what a good k number of clusters would be based on the sum of squared distance (SSE) between data points 
and their assigned clusters’ centroids. 
We pick k at the spot where SSE starts to flatten out and forming an elbow. 
We’ll use the geyser dataset and evaluate SSE for different values of k and see where the curve might form an elbow and flatten out.

Within-Cluster-Sum of Squared Errors sounds a bit complex. 
Let’s break it down:
•	The Squared Error for each point is the square of the distance of the point from its representation i.e. its predicted cluster center.
•	The WSS score is the sum of these Squared Errors for all the points.
•	Any distance metric like the Euclidean Distance or the Manhattan Distance can be used.


Silhouette Analysis
Silhouette analysis can be used to determine the degree of separation between clusters. For each sample:
Compute the average distance from all data points in the same cluster (ai).
Compute the average distance from all data points in the closest cluster (bi).
Compute the coefficient:
coeff=(bi-ai)/max(ai,bi)

The coefficient can take values in the interval [-1, 1].
If it is 0 –> the sample is very close to the neighboring clusters.
It it is 1 –> the sample is far away from the neighboring clusters.
It it is -1 –> the sample is assigned to the wrong clusters.

Therefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters. 
We’ll use here geyser dataset again because its cheaper to run the silhouette analysis and it is actually obvious that there is most likely
 only two groups of data points.


Inertia: Inertia measures the internal cluster sum of squares (sum of squares is the sum of all residuals). 
Inertia is utilized to measure how related clusters are amongst themselves, the lower the inertia score the better. 
HOWEVER, it is important to note that inertia heavily relies on the assumption that the clusters are convex (of spherical shape). 
DBSCAN does not necessarily divide data into spherical clusters, therefore inertia is not a good metric to use for evaluating DBSCAN models 
(which is why I did not include inertia in the code above). 
Inertia is more often used in other clustering methods, such as K-means clustering.
 --------------------------------------------------------------------------------------------------------------------------

when should you use Kmeans?
if the features are numerical then we go with kmeans.

when should you use Kmodes?
if the data has catogorical variables and we like to have certain number of clusters then we go with kmodes

when we go with Hirarcheal?
when we want the data to be clustered in a Hirarcheal manner.

when do we go with GMM(Gaussian mixture model)?
if the features are numerical and want to cluster the data with probability then we go with GMM.
when should you use DBScan?
if you want to cluster without specifying the value of K then we go with DBScan.

-------------------------------------------------------------------------------------------------------------------------------
